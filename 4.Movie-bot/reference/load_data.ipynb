{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LHW2 : Maximum Entropy Markov Models\n",
    "In this homework, you will be using a first-order maximum entropy Markov model for part-of-speech tagging. You will be implementing Viterbi decoding, featurizing the text for the MEMM, and performing ablation tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We'll begin by importing the needed models and providing some skeleton code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# Dictionary to store indices for each feature\n",
    "feature_vocab = {}\n",
    "\n",
    "# Dictionary to store the indices of each POS tag\n",
    "label_vocab = {}\n",
    "\n",
    "\n",
    "# load up any external resources here\n",
    "def initialize():\n",
    "    \"\"\" \n",
    "        :return (dictionary data - any format that you wish, which\n",
    "        can be reused in get_features below)\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "        load data from filename and return a list of lists\n",
    "        all_toks = [toks1, toks2, ...] where toks1 is\n",
    "                a sequence of words (sentence)\n",
    "\n",
    "        all_labs = [labs1, labs2, ...] where labs1 is a sequence of\n",
    "                labels for the corresponding sentence\n",
    "    \"\"\"\n",
    "    file = open(filename)\n",
    "    all_toks = []\n",
    "    all_labs = []\n",
    "    toks = []\n",
    "    labs = []\n",
    "    for line in file:\n",
    "        # Skip the license\n",
    "        if \"This data is licensed from\" in line:\n",
    "            continue\n",
    "        cols = line.rstrip().split(\"\\t\")\n",
    "        if len(cols) < 2:\n",
    "            all_toks.append(toks)\n",
    "            all_labs.append(labs)\n",
    "            toks = []\n",
    "            labs = []\n",
    "            continue\n",
    "        toks.append(cols[0])\n",
    "        labs.append(cols[1])\n",
    "\n",
    "    if len(toks) > 0:\n",
    "        all_toks.append(toks)\n",
    "        all_labs.append(labs)\n",
    "\n",
    "    return all_toks, all_labs\n",
    "\n",
    "\n",
    "def print_message(m):\n",
    "    num_stars = 10\n",
    "    if verbose:\n",
    "        print(\"*\" * num_stars + m + \"*\" * num_stars)\n",
    "\n",
    "\n",
    "def decode(Y_pred):\n",
    "    \"\"\"\n",
    "        select the decoding algorithm\n",
    "    \"\"\"\n",
    "    if use_greedy:\n",
    "        return greedy_decode(Y_pred)\n",
    "    else:\n",
    "        return viterbi_decode(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell there are some constants that will be used for the MEMM. Feel free to change the values of any of the constants or flags if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control variables, use for debugging and testing\n",
    "verbose = True\n",
    "use_greedy = False\n",
    "\n",
    "# Minimum number of observations for a feature to be included in model\n",
    "MINIMUM_FEAT_COUNT = 2 \n",
    "# L2 regularization strength; range is (0,infinity), with stronger regularization -> 0 (in this package)\n",
    "L2_REGULARIZATION_STRENGTH = 1e0 \n",
    "\n",
    "TRAINING_FILE = 'wsj.pos.train'\n",
    "DEVELOPMENT_FILE = 'wsj.pos.dev'\n",
    "TEST_FILE = 'wsj.pos.test'\n",
    "OUTPUT_CSV = 'predictions.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Viterbi Decoding\n",
    "Below we have the function for implementing greedy decoding, which we'd like to improve upon by using Viterbi decoding. In the cell following, implement `viterbi_decode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(Y_pred):\n",
    "    \"\"\" \n",
    "        greedy decoding to get the sequence of label predictions\n",
    "    \"\"\"\n",
    "    cur = label_vocab[\"START\"]\n",
    "    preds = []\n",
    "    for i in range(len(Y_pred)):\n",
    "        pred = np.argmax(Y_pred[i, cur])\n",
    "        preds.append(pred)\n",
    "        cur = pred\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decode(Y_pred):\n",
    "    \"\"\"\n",
    "        :return\n",
    "        list of POS tag indices, defined in label_vocab,\n",
    "        in order of sequence\n",
    "\n",
    "        :param\n",
    "        Y_pred: Tensor of shape N * M * L, where\n",
    "        N is the number of words in the current sequence,\n",
    "        L is the number of POS tag labels\n",
    "        M = L + 1, where M is the number of possible previous labels\n",
    "        which includes L + \"START\"\n",
    "\n",
    "        M_{x,y,z} is the probability of a tag (label_vocab[current_tag] = z)\n",
    "        given its current position x in the sequence and\n",
    "        its previous tag (label_vocab[previous_tag] = y)\n",
    "\n",
    "        Consider the sentence - \"I have a dog\". Here, N = 4.\n",
    "        Assume that there are only 3 possible tags: {PRP, VBD, NN}\n",
    "        M_{0, 3, 2} would give you the probability of \"I\" being a \"NN\" if\n",
    "        it is preceded by \"START\". \"START\" is the last index of all lablels,\n",
    "        and in our example denoted by 3.\n",
    "    \"\"\"\n",
    "    (N, M, L) = Y_pred.shape\n",
    "\n",
    "    # list of POS tag indices to be returned\n",
    "    path = []\n",
    "\n",
    "    \"\"\"\n",
    "        Type your code below\n",
    "\n",
    "        Hints:\n",
    "        step 1. construct a Viterbi matrix to store the\n",
    "        intermediate joint probabilities\n",
    "        step 2. construct a matrix to store the backpointers\n",
    "        to retrieve the path\n",
    "        step 3. decode and return the path from the Viterbi matrix\n",
    "        Note: Probabilities get smaller as you multiply them, and may\n",
    "        vanish. Remember how we have treated chained probabilities\n",
    "        this entire class.\n",
    "\n",
    "    \"\"\"\n",
    "    viterbi = np.zeros((L,N))\n",
    "    backpointer = np.zeros((L,N))\n",
    "    \n",
    "    for s in range(L):\n",
    "        viterbi[s,0] = math.log(Y_pred[0][-1][s])\n",
    "        backpointer[s,0] = 0\n",
    "    for t in range(1,N):\n",
    "        for s in range(L):\n",
    "            prob = [viterbi[prev_tag,t-1]+math.log(Y_pred[t][prev_tag][s]) for prev_tag in range(L)]\n",
    "            viterbi[s,t] = np.max(prob)\n",
    "            backpointer[s,t] = np.argmax(prob)\n",
    "\n",
    "    bestpathprob = np.max(viterbi[:,N-1])\n",
    "    bestpathpointer = np.argmax(viterbi[:,N-1])\n",
    "    \n",
    "    tag = bestpathpointer\n",
    "    path.append(tag)\n",
    "    for t in range(N-1,0,-1):\n",
    "        tag = int(backpointer[tag,t])\n",
    "        path.append(tag)\n",
    "    \n",
    "    return path[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features\n",
    "In the `get_features` function below, add features for the maximum entropy Markov model to use, much like in short homework 1. A baseline set of features utilizing unigrams, the tag of the previous word, and the end of the sequence is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/waynewu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(index, sequence, tag_index_1, data):\n",
    "    \"\"\"\n",
    "        :params\n",
    "        index: the index of the current word in the sequence to featurize\n",
    "\n",
    "        sequence: the sequence of words for the entire sentence\n",
    "\n",
    "        tag_index_1: gives you the POS\n",
    "        tag for the previous word.\n",
    "\n",
    "        data: the data you have built in initialize()\n",
    "        to enrich your feature representation. Use data as you see fit.\n",
    "\n",
    "        :return (feature dictionary)\n",
    "        features are in the form of {feature_name: feature_value}\n",
    "        Calculate the values of each feature for a given\n",
    "        word in a sequence.\n",
    "\n",
    "        The current implementation returns the following as features:\n",
    "        the current word, the tag of the previous word, and whether an\n",
    "        index the the last in the sequence.\n",
    "\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    word = sequence[index].lower()\n",
    "    \n",
    "    features[\"UNIGRAM_%s\" % word] = 1\n",
    "    features[\"PREVIOUS_TAG_%s\" % tag_index_1] = 1\n",
    "    if index == len(sequence) - 1:\n",
    "        features[\"LAST_WORD_IN_SEQUENCE\"] = 1\n",
    "\n",
    "    \"\"\"\n",
    "        Create your own features here\n",
    "    \n",
    "    \"\"\"\n",
    "    #ti = current_tag and wi−2,wi-1\n",
    "    #ti = current_tag and wi+1,wi+2\n",
    "    for i in range(1,3):\n",
    "        if index>i:\n",
    "            features[\"PREVIOUS_\"+str(i)+\"_UNIGRAM_%s\" % sequence[index-i].lower()] = 1\n",
    "        if index+i < len(sequence):\n",
    "            features[\"NEXT_\"+str(i)+\"_UNIGRAM_%s\" % sequence[index+i].lower()] = 1\n",
    "    \n",
    "    #ti contains at least 1-4 character at front\n",
    "    if re.search(r'^[a-z]{1}', word) != None:\n",
    "        features[\"PREFIX_%s\" % word[:1]] = 1\n",
    "    if re.search(r'^[a-z]{2}', word) != None:\n",
    "        features[\"PREFIX_%s\" % word[:2]] = 1\n",
    "    if re.search(r'^[a-z]{3}', word) != None:\n",
    "        features[\"PREFIX_%s\" % word[:3]] = 1\n",
    "    if re.search(r'^[a-z]{4}', word) != None:\n",
    "        features[\"PREFIX_%s\" % word[:4]] = 1\n",
    "    \n",
    "    #ti contains at least 1-4 character at end\n",
    "    if re.search(r'[a-z]{1}$', word) != None:\n",
    "        features[\"PREFIX_%s\" % word[:-1]] = 1\n",
    "    if re.search(r'[a-z]{2}$', word) != None:\n",
    "        features[\"PREFIX_%s\" % word[:-2]] = 1\n",
    "    if re.search(r'[a-z]{3}$', word) != None:\n",
    "        features[\"PREFIX_%s\" % word[:-3]] = 1\n",
    "    if re.search(r'[a-z]{4}$', word) != None:\n",
    "        features[\"PREFIX_%s\" % word[:-4]] = 1\n",
    "    \n",
    "    #ti contains more than one upper-case\n",
    "    if re.search(r'[A-Z]+', sequence[index]) != None:\n",
    "        features[\"CONTAIN_ONE_UPPERCASE\"] = 1\n",
    "    #ti contains number or hyphen\n",
    "    if re.search(r'[0-9\\-]+', word) != None:\n",
    "        features[\"CONTAIN_NUMBER_OR_HYPHEN\"] = 1\n",
    "    #ti contains '$%{}&()-`'\n",
    "    if re.search(r'[$%{}&()-]+', word) != None:\n",
    "        features[\"CONTAIN_NUMBER_OR_HYPHEN\"] = 1\n",
    "        \n",
    "    #ti word shape XX-XXX, XX.XX, ''s', ',', end with s, end with ed, end with ing\n",
    "    if re.search(r'(\\w+)-(\\w+)', word) != None:\n",
    "        features[\"WORD_SHAPE_XX-XXX\"] = 1\n",
    "    if re.search(r'(\\d*).(\\d*)', word) != None:\n",
    "        features[\"WORD_SHAPE_XX.XX\"] = 1\n",
    "    if re.search(r'\\'s', word) != None:\n",
    "        features[\"WORD_SHAPE_'s\"] = 1\n",
    "    if re.search(r',', word) != None:\n",
    "        features[\"WORD_SHAPE_,\"] = 1\n",
    "    if re.search(r's$', word) != None:\n",
    "        features[\"END_WITH_S\"] = 1\n",
    "    if re.search(r'ed$', word) != None:\n",
    "        features[\"END_WITH_ED\"] = 1\n",
    "    if re.search(r'ing$', word) != None:\n",
    "        features[\"END_WITH_ING\"] = 1\n",
    "    \n",
    "    #ti in STOPWORDS\n",
    "    if word in STOPWORDS:\n",
    "        features[\"STOPWORDS\"] =1\n",
    "    \n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both Viterbi decoding implemented and features added, we can now build and train our MEMM! Run the cell below to define all the necessary functions to complete our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(filename, data):\n",
    "    \"\"\"\n",
    "        train a model to generate Y_pred\n",
    "    \"\"\"\n",
    "    all_toks, all_labs = load_data(filename)\n",
    "    vocab = {}\n",
    "\n",
    "    # X_verbose is a list of feature objects for the entire train dataset.\n",
    "    # Each feature object is a dictionary defined by\n",
    "    # get_features and corresponds to a word\n",
    "    # Y_verbose is a list of labels for all words in the entire train dataset\n",
    "    X_verbose = []\n",
    "    Y_verbose = []\n",
    "\n",
    "    feature_counts=Counter()\n",
    "\n",
    "    for i in range(len(all_toks[:10000])):\n",
    "        toks = all_toks[i]\n",
    "        labs = all_labs[i]\n",
    "        for j in range(len(toks)):\n",
    "            prev_lab = labs[j - 1] if j > 0 else \"START\"\n",
    "            feats = get_features(j, toks, prev_lab, data)\n",
    "            X_verbose.append(feats)\n",
    "            Y_verbose.append(labs[j])\n",
    "\n",
    "            for feat in feats:\n",
    "                feature_counts[feat]+=1\n",
    "\n",
    "    # construct label_vocab (dictionary) and feature_vocab (dictionary)\n",
    "    # label_vocab[pos_tag_label] = index_for_the_pos_tag\n",
    "    # feature_vocab[feature_name] = index_for_the_feature\n",
    "    feature_id = 1\n",
    "    label_id = 0\n",
    "\n",
    "    # create unique integer ids for each label and each feature above the minimum count threshold\n",
    "    for i in range(len(X_verbose)):\n",
    "        feats = X_verbose[i]\n",
    "        true_label = Y_verbose[i]\n",
    "\n",
    "        for feat in feats:\n",
    "            if feature_counts[feat] >= MINIMUM_FEAT_COUNT:\n",
    "                if feat not in feature_vocab:\n",
    "                    feature_vocab[feat] = feature_id\n",
    "                    feature_id += 1\n",
    "        if true_label not in label_vocab:\n",
    "            label_vocab[true_label] = label_id\n",
    "            label_id += 1\n",
    "\n",
    "    # START has last id\n",
    "    label_vocab[\"START\"] = label_id\n",
    "\n",
    "    # create train input and output to train the logistic regression model\n",
    "    # create sparse input matrix\n",
    "\n",
    "    # X is documents x features empty sparse matrix\n",
    "    X = sparse.lil_matrix((len(X_verbose), feature_id))\n",
    "    Y = []\n",
    "\n",
    "    print_message(\"Number of features: %s\" % len(feature_vocab))\n",
    "    for i in range(len(X_verbose)):\n",
    "        feats = X_verbose[i]\n",
    "        true_label = Y_verbose[i]\n",
    "        for feat in feats:\n",
    "            if feat in feature_vocab:\n",
    "                X[i, feature_vocab[feat]] = feats[feat]\n",
    "        Y.append(label_vocab[true_label])\n",
    "\n",
    "    # fit model\n",
    "    log_reg = linear_model.LogisticRegression(C=L2_REGULARIZATION_STRENGTH, penalty='l2')\n",
    "    log_reg.fit(sparse.coo_matrix(X), Y)\n",
    "\n",
    "    return log_reg\n",
    "\n",
    "\n",
    "def test_dev(filename, log_reg, data):\n",
    "    \"\"\"\n",
    "        predict labels using the trained model\n",
    "        and evaluate the performance of model\n",
    "    \"\"\"\n",
    "    all_toks, all_labs = load_data(filename)\n",
    "\n",
    "    # possible output labels = all except START\n",
    "    L = len(label_vocab) - 1\n",
    "\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    num_features = len(feature_vocab) + 1\n",
    "\n",
    "    # for each sequence (sentence) in the test dataset\n",
    "    for i in range(len(all_toks[:3000])):\n",
    "\n",
    "        toks = all_toks[i]\n",
    "        labs = all_labs[i]\n",
    "\n",
    "        if len(toks) == 0:\n",
    "            continue\n",
    "\n",
    "        N = len(toks)\n",
    "\n",
    "        X_test = []\n",
    "        # N x prev_tag x cur_tag\n",
    "        Y_pred = np.zeros((N, L + 1, L))\n",
    "\n",
    "        # vector of true labels\n",
    "        Y_test = []\n",
    "\n",
    "        # for each token (word) in the sentence\n",
    "        for j in range(len(toks)):\n",
    "\n",
    "            true_label = labs[j]\n",
    "            Y_test.append(true_label)\n",
    "\n",
    "            # for each preceding tag of the word\n",
    "            for possible_previous_tag in label_vocab:\n",
    "                X = sparse.lil_matrix((1, num_features))\n",
    "\n",
    "                feats = get_features(j, toks, possible_previous_tag, data)\n",
    "                valid_feats = {}\n",
    "                for feat in feats:\n",
    "                    if feat in feature_vocab:\n",
    "                        X[0, feature_vocab[feat]] = feats[feat]\n",
    "\n",
    "                # update Y_pred with the probabilities of all current tags\n",
    "                # given the current word, previous tag and other data/feature\n",
    "                prob = log_reg.predict_proba(X)\n",
    "                Y_pred[j, label_vocab[possible_previous_tag]] = prob\n",
    "\n",
    "        # decode to get the predictions\n",
    "        predictions = decode(Y_pred)\n",
    "\n",
    "        # evaluate the performance of the model by checking predictions\n",
    "        # against true labels\n",
    "        for k in range(len(predictions)):\n",
    "            if Y_test[k] in label_vocab and predictions[k] == label_vocab[Y_test[k]]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "        print(\"Development Accuracy: %.3f (%s/%s).\" % (correct / total, correct, total), end=\"\\r\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ablation Tests\n",
    "Run the the model with all of your features included in the cell below, and then once each time you remove a feature. Record your results in a table in a pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Initialize Data**********\n",
      "**********Train Model**********\n",
      "**********Number of features: 59743**********\n"
     ]
    }
   ],
   "source": [
    "feature_vocab = {}\n",
    "print_message(\"Initialize Data\")\n",
    "data = initialize()\n",
    "print_message(\"Train Model\")\n",
    "log_reg = train(TRAINING_FILE, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Test Model**********\n",
      "Development Accuracy: 0.955 (52656.0/55166.0).\r"
     ]
    }
   ],
   "source": [
    "print_message(\"Test Model\")\n",
    "test_dev(DEVELOPMENT_FILE, log_reg, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kaggle Submission\n",
    "After you've implemented Viterbi decoding and add features of your own, first run the block below to define necessary functions, and the run the block after to generate tags on the test dataset. Submit the created csv onto kaggle at the link https://www.kaggle.com/t/a650043b2d564412b388c3bd352e25dd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(filename):\n",
    "    \"\"\"\n",
    "        load data from filename and return a list of lists\n",
    "        all_toks = [toks1, toks2, ...] where toks1 is\n",
    "                a sequence of words (sentence)\n",
    "    \"\"\"\n",
    "    file = open(filename)\n",
    "    all_toks = []\n",
    "    toks = []\n",
    "    for line in file:\n",
    "        # Skip the license\n",
    "        if \"This data is licensed from\" in line:\n",
    "            continue\n",
    "        col = line.rstrip().split(\"\\t\")[0]\n",
    "        if col == '':\n",
    "            all_toks.append(toks)\n",
    "            toks = []\n",
    "            continue\n",
    "        toks.append(col)\n",
    "\n",
    "    if len(toks) > 0:\n",
    "        all_toks.append(toks)\n",
    "\n",
    "    return all_toks\n",
    "\n",
    "\n",
    "def write_predictions(datafile, outputfile, log_reg, data):\n",
    "    \"\"\"\n",
    "        predict labels using the trained model\n",
    "        and evaluate the performance of model\n",
    "    \"\"\"\n",
    "    all_toks = load_test(datafile)\n",
    "    index_to_label = {v: k for k, v in label_vocab.items()}\n",
    "\n",
    "    # possible output labels = all except START\n",
    "    L = len(label_vocab) - 1\n",
    "    num_features = len(feature_vocab) + 1\n",
    "    n = 1\n",
    "\n",
    "    with open(outputfile, 'w') as f:\n",
    "        f.write('tokens,labels\\n')\n",
    "        # for each sequence (sentence) in the test dataset\n",
    "        for i in range(len(all_toks[:100])):\n",
    "            toks = all_toks[i]\n",
    "            if len(toks) == 0:\n",
    "                continue\n",
    "            N = len(toks)\n",
    "\n",
    "            # N x prev_tag x cur_tag\n",
    "            Y_pred = np.zeros((N, L + 1, L))\n",
    "\n",
    "            # for each token (word) in the sentence\n",
    "            for j in range(len(toks)):\n",
    "\n",
    "                # for each preceding tag of the word\n",
    "                for possible_previous_tag in label_vocab:\n",
    "                    X = sparse.lil_matrix((1, num_features))\n",
    "\n",
    "                    feats = get_features(j, toks, possible_previous_tag, data)\n",
    "                    valid_feats = {}\n",
    "                    for feat in feats:\n",
    "                        if feat in feature_vocab:\n",
    "                            X[0, feature_vocab[feat]] = feats[feat]\n",
    "\n",
    "                    # update Y_pred with the probabilities of all current tags\n",
    "                    # given the current word, previous tag and other data/feature\n",
    "                    prob = log_reg.predict_proba(X)\n",
    "                    Y_pred[j, label_vocab[possible_previous_tag]] = prob\n",
    "\n",
    "            # decode to get the predictions\n",
    "            predictions = decode(Y_pred)\n",
    "            for k in range(len(predictions)):\n",
    "                f.write(str(n) + ',' + index_to_label[predictions[k]] + '\\n')\n",
    "                n += 1\n",
    "            f.write('\\n')\n",
    "            print(\"Generation predictions, %.2f%% complete.\" % ((i+1)/len(all_toks[:100])*100), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation predictions, 100.00% complete.\r"
     ]
    }
   ],
   "source": [
    "write_predictions(TEST_FILE, OUTPUT_CSV, log_reg, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
