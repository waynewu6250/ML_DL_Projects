{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Chatbot\n",
    "## 2018.11.20\n",
    "Homework Instruction:\n",
    "\n",
    "In this homework you will be creating a Chatbot using a sequence-to-sequence model. You are allowed to work in\n",
    "groups of up to 2 students. This homework is of an open format; all we will be providing you with is the data. It is up\n",
    "to you to pre-process the data, build the seq-2-seq model with keras, and train the model. You will be submitting your\n",
    "code and write-up containing the three sections deﬁned below.\n",
    "\n",
    "<img src=\"https://cdn.technologyadvice.com/wp-content/uploads/2018/02/friendly-chatbot-700x408.jpg\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup:\n",
    "## Prepare data for neural network\n",
    "We'll begin by importing the needed models and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import LSTM, Embedding \n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Flatten, Reshape\n",
    "\n",
    "# hyperparameter\n",
    "mxlen = 20 # Max length for a sequence of tokens\n",
    "batch_size = 128  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "char_dim= 30 # Embedding size\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "word2id = {}\n",
    "id2word = {}\n",
    "\n",
    "data_path = \"movie_lines.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "        Load the movie_lines.tsv file which contains the data. \n",
    "        The ﬁle has ﬁve tab separated columns containing the following ﬁelds:\n",
    "        1. lineID\n",
    "        2. characterID (who uttered this phrase)\n",
    "        3. movieID\n",
    "        4. character name\n",
    "        5. text of the utterance\n",
    "        \n",
    "        all_lineids = [lineids1, lineids2, ...] where lineids is a sequence of \n",
    "            utterances for one movie.\n",
    "        all_ids = [ids1, ids2, ...] where ids is a sequence of \n",
    "            character ids for one movie.\n",
    "        all_toks = [toks1, toks2, ...] where toks is a sequence of \n",
    "            words (sentences) for one movie.\n",
    "    \n",
    "    \"\"\"\n",
    "    file = open(filename)\n",
    "    all_lineids = []\n",
    "    all_ids = []\n",
    "    all_toks = []\n",
    "    \n",
    "    lineids = []\n",
    "    ids = []\n",
    "    toks = []\n",
    "    mid = \"m0\"\n",
    "    \n",
    "    for line in file:\n",
    "        cols = line.rstrip().split(\"\\t\")\n",
    "        #Only get the data with entire five columns\n",
    "        if len(cols) < 5:\n",
    "            continue\n",
    "        \n",
    "        if cols[2] != mid:\n",
    "            all_lineids.append(lineids)\n",
    "            all_ids.append(ids)\n",
    "            all_toks.append(toks)\n",
    "            \n",
    "            #Restart new movie data\n",
    "            lineids = [int(cols[0].strip('\"L'))]\n",
    "            ids = [int(cols[1].strip('u'))]\n",
    "            toks = [cols[4]]\n",
    "            mid = cols[2]\n",
    "            continue\n",
    "        \n",
    "        lineids.append(int(cols[0].strip('\"L')))\n",
    "        ids.append(int(cols[1].strip('u')))\n",
    "        toks.append(cols[4])\n",
    "    \n",
    "    if len(toks) > 0:\n",
    "        all_lineids.append(lineids)\n",
    "        all_ids.append(ids)\n",
    "        all_toks.append(toks)\n",
    "    \n",
    "    return all_lineids, all_ids, all_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lineids, all_ids, all_toks = load_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Text Prepare\n",
    "\n",
    "We are only providing you with the utterances. It is up to you to process the utterances into a format that can be fed\n",
    "into the model. <br>\n",
    "For information on how to process the data take a look at this paper: A Neural Conversation Model. <br>\n",
    "Some ideas for pre-processing may include removing infrequent words and replacing them with the <UNK> token.\n",
    "In your write-up include what you did to pre-process the dialogue data.\n",
    "    \n",
    "** 1) Remove bad symbols and tokenization <br> **\n",
    "** 2) *Count the words that appear only onces and replace them with <UNK\\> <br> **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Remove bad symbols and tokenization\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified string tokens \n",
    "                [tok1, tok2 , ...] which is a single sentence from one character\n",
    "    \"\"\"\n",
    "    tok = [\"<START>\"]\n",
    "    text = text.lower() # lowercase text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(BAD_SYMBOLS_RE, '', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    tok += (text.split()+[\"<EOS>\"])\n",
    "    \n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Dictionary of all words from train corpus with their counts.\n",
    "#    Dictionary of all words with its ids\n",
    "def count_words(all_toks):\n",
    "    \"\"\"\n",
    "    count:\n",
    "    { tok1: count1, tok2: count2, ...}\n",
    "    word2id:\n",
    "    { tok1: id1, tok2: id2, ...}\n",
    "    id2word:\n",
    "    { id1: tok1, id2: tok2, ...}\n",
    "    \n",
    "    \"\"\"\n",
    "    count = {}\n",
    "    \n",
    "    # Special Tokens\n",
    "    word2id[\"<START>\"] = 0\n",
    "    word2id[\"<EOS>\"] = 1\n",
    "    word2id[\"<UNK>\"] = 2\n",
    "    word2id[\"<PAD>\"] = 3\n",
    "    index = 4\n",
    "    \n",
    "    for toks in all_toks:\n",
    "        for tok in toks:\n",
    "            for word in tok:\n",
    "                # Count the words\n",
    "                if not word in count:\n",
    "                    count[word] = 1\n",
    "                else:\n",
    "                    count[word] += 1\n",
    "                # Make dictionary\n",
    "                if not word in word2id:\n",
    "                    word2id[word] = index\n",
    "                    index += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenize(all_toks):\n",
    "    \n",
    "    \"\"\"\n",
    "     all_toks_new: \n",
    "     [\n",
    "        movie 0:[ line 0: [tok1, tok2, ...],\n",
    "                  line 1: [tok1, tok2, ...],\n",
    "                  ... ]\n",
    "        movie 1:[ line 0: [tok1, tok2, ...],\n",
    "                  line 1: [tok1, tok2, ...],\n",
    "                  ... ]\n",
    "        movie 2:[ line 0: [tok1, tok2, ...],\n",
    "                  line 1: [tok1, tok2, ...],\n",
    "                  ... ]\n",
    "        ...\n",
    "\n",
    "     ]\n",
    "     \n",
    "     scarce_words_counts: a list of words that appear only once.\n",
    "     [tok1, tok2, tok3, ...]\n",
    "     \n",
    "    \"\"\"\n",
    "    all_toks_new = []\n",
    "\n",
    "    # Prepare the text\n",
    "    for toks in all_toks:\n",
    "        toks = [text_prepare(x) for x in toks]\n",
    "        all_toks_new.append(toks)\n",
    "\n",
    "    # Count the words that appears only once.\n",
    "    words_counts = count_words(all_toks_new)\n",
    "    scarce_words_counts = [x[0] for x in sorted(words_counts.items(), key = lambda x: x[1], reverse=True) if x[1] == 1]\n",
    "    \n",
    "    # Remove scarce words in word2id dictionary and reindex all words\n",
    "    for word in scarce_words_counts:\n",
    "        del word2id[word]\n",
    "    \n",
    "    return all_toks_new, scarce_words_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.19 s, sys: 122 ms, total: 3.31 s\n",
      "Wall time: 3.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_toks_new, scarce_words_counts = text_tokenize(all_toks)\n",
    "word2id = {key: i for i, key in enumerate(word2id.keys())}\n",
    "id2word = {i:symbol for symbol, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration (per 100 movies):  0\n",
      "Iteration (per 100 movies):  1\n",
      "Iteration (per 100 movies):  2\n",
      "Iteration (per 100 movies):  3\n",
      "Iteration (per 100 movies):  4\n",
      "Iteration (per 100 movies):  5\n",
      "Iteration (per 100 movies):  6\n",
      "CPU times: user 1h 7min 15s, sys: 18.1 s, total: 1h 7min 33s\n",
      "Wall time: 1h 8min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################################\n",
    "# Default: Skip, load the data directly\n",
    "# Replace the word with <UNK> that appears only once. This should take a while\n",
    "#for movie in all_toks_new:\n",
    "for i in range(len(all_toks_new)):\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration (per 100 movies): \",int(i/100))\n",
    "    for toks in all_toks_new[i]:\n",
    "        for j in range(len(toks)):\n",
    "            if toks[j] in scarce_words_counts:\n",
    "                toks[j] = \"<UNK>\"\n",
    "\n",
    "file1=open(\"all_toks_new.bin\",\"wb\")\n",
    "file2=open(\"word2id.bin\",\"wb\")\n",
    "pickle.dump(all_toks_new,file1)\n",
    "pickle.dump(word2id,file2)\n",
    "file1.close()\n",
    "file2.close()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data to save time\n",
    "file1=open(\"all_toks_new.bin\",\"rb\")\n",
    "file2=open(\"word2id.bin\",\"rb\")\n",
    "all_toks_new=pickle.load(file1)\n",
    "word2id=pickle.load(file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Vector and Padding \n",
    "In the following steps, we first make the word into id vectors which will be fed into keras embedding layers. <br>\n",
    "As for Padding: For the tokens into ids we have processed, we wish to pad all the sentences to a fixed mxlen, where we define 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(all_toks_new, mxlen):\n",
    "    \"\"\"\n",
    "    all_toks_new: (each with same length mxlen)\n",
    "     [\n",
    "        movie 0:[ line 0: [id1, id2, ...],\n",
    "                  line 1: [id1, id2, ...],\n",
    "                  ... ]\n",
    "        movie 1:[ line 0: [id1, id2, ...],\n",
    "                  line 1: [id1, id2, ...],\n",
    "                  ... ]\n",
    "        movie 2:[ line 0: [id1, id2, ...],\n",
    "                  line 1: [id1, id2, ...],\n",
    "                  ... ]\n",
    "        ...\n",
    "\n",
    "     ]\n",
    "    \"\"\"\n",
    "    for movie in all_toks_new:\n",
    "        for i in range(len(movie)):\n",
    "            #token to id\n",
    "            movie[i] = [word2id[x] for x in movie[i]]\n",
    "            #padding\n",
    "            if len(movie[i]) < mxlen:\n",
    "                movie[i] += [3] * (mxlen-len(movie[i])) \n",
    "            movie[i] = np.array(movie[i][:mxlen])\n",
    "    \n",
    "    return all_toks_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_toks_new = pad(all_toks_new, mxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Make encoding and decoding data\n",
    "Turn the data into three main data forms: <br>\n",
    "**input_tokens, output_tokens, output_target_tokens**\n",
    "\n",
    "\n",
    "for training a basic LSTM-based Seq2Seq model to predict output_target_tokens given input_tokens and output_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_conv(ids, toks):\n",
    "    \"\"\"\n",
    "    Separate the sequence of characters and their words if they utter continuously without waiting for the other to speak\n",
    "    For example:\n",
    "    ids = [2, 0, 2, 0, 2, 0, 0 ,2]\n",
    "    toks = [tok1, tok2, tok3, tok4, tok5, tok6, tok7, tok8]\n",
    "    sep_toks = [[tok1, tok2, tok3, tok4, tok5, tok6], [tok7, tok8]]\n",
    "    \n",
    "    \"\"\"\n",
    "    sep_toks = []\n",
    "    for i in range(len(ids)):\n",
    "        if i == 0:\n",
    "            temp = ids[i]\n",
    "            idx = i\n",
    "        else:\n",
    "            if temp == ids[i]:\n",
    "                sep_toks.append(toks[idx:i])\n",
    "                idx = i\n",
    "            temp = ids[i]\n",
    "        \n",
    "        if i == (len(ids)-1):\n",
    "            sep_toks.append(toks[idx:len(ids)])\n",
    "    \n",
    "    return sep_toks            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(all_lineids, all_ids, all_toks_new):\n",
    "    \"\"\"\n",
    "    Transform our original data with all dialogues all_toks_new into training data (input_tokens, output_tokens)\n",
    "    \n",
    "    A movie can be seen as an entity with sequential characters' conversations.\n",
    "    We deem a conversation end when two line ids are not consecutive.\n",
    "    for example, if a lineid sequence is [242, 241, 237, 236, 235]\n",
    "    we can make it into two conversations: [242, 241], [237, 236, 235]\n",
    "    \n",
    "    After specifying the conversations, we can then prepare the training data as follows:\n",
    "    for two conversations: [242, 241], [237, 236, 235] and corresponding token sequence is [toks1, toks2], [toks3, toks4, toks5]\n",
    "    we make input_tokens as [toks2], [toks4, toks5]\n",
    "            output_tokens as [toks1], [toks3, toks4]\n",
    "    \n",
    "    \n",
    "    Then we combine all tokens input-output pairs of every conversation in every movie.\n",
    "    so we will have\n",
    "    input_tokens = [toks2, toks4, toks5, toks7, ...]\n",
    "    output_tokens = [toks1, toks3, toks4, toks6, ...]\n",
    "    \n",
    "    Finally we wish to have our target output tokens to be almost same as output_tokens with each data ahead by one timestep.\n",
    "    output_target_tokens = [toks1, toks3, toks4, toks6, ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    input_tokens = []\n",
    "    output_tokens = []\n",
    "    \n",
    "    N = len(all_lineids) #number of movies\n",
    "\n",
    "    for i in range(N):\n",
    "        #For a single movie\n",
    "        movie = all_lineids[i]\n",
    "        for j in range(len(movie)):\n",
    "            if j == 0:\n",
    "                temp = movie[j]\n",
    "                idx = j\n",
    "            else:\n",
    "                if (temp-movie[j]) is not 1:\n",
    "                    sep_toks = separate_conv(all_ids[i][idx:j], all_toks_new[i][idx:j])\n",
    "                    for toks in sep_toks:\n",
    "                        input_tokens += toks[1:]\n",
    "                        output_tokens += toks[:-1]\n",
    "\n",
    "                    idx = j\n",
    "                temp = movie[j]\n",
    "\n",
    "            #Last Sequence\n",
    "            if j == len(movie)-1:\n",
    "                sep_toks = separate_conv(all_ids[i][idx:len(movie)], all_toks_new[i][idx:len(movie)])\n",
    "                for toks in sep_toks:\n",
    "                    input_tokens += toks[1:]\n",
    "                    output_tokens += toks[:-1]\n",
    "    \n",
    "    output_target_tokens = [0]*len(output_tokens)\n",
    "    for i in range(len(output_tokens)):\n",
    "        output_target_tokens[i] = np.concatenate((output_tokens[i][1:], [3]))\n",
    "            \n",
    "    return input_tokens, output_tokens, output_target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c0f7b133aa3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_target_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_lineids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_toks_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_target_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_target_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-48d945d6b3cf>\u001b[0m in \u001b[0;36mmake_data\u001b[0;34m(all_lineids, all_ids, all_toks_new)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmovie\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0msep_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseparate_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_toks_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtoks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msep_toks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                         \u001b[0minput_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtoks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "input_tokens, output_tokens, output_target_tokens = make_data(all_lineids, all_ids, all_toks_new)\n",
    "input_tokens = np.asarray(input_tokens)\n",
    "output_tokens = np.asarray(output_tokens)\n",
    "output_target_tokens = np.asarray(output_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data = np.zeros(\n",
    "    (len(input_tokens[:10000]), mxlen, len(word2id)),\n",
    "    dtype='float32')\n",
    "for i, target in enumerate(output_target_tokens[:10000]):\n",
    "    for t, idd in enumerate(target):\n",
    "        decoder_target_data[i, t, idd] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235361, 20)\n",
      "(235361, 20)\n",
      "(10000, 20, 33983)\n",
      "33983\n"
     ]
    }
   ],
   "source": [
    "print(input_tokens.shape)\n",
    "print(output_target_tokens.shape)\n",
    "print(decoder_target_data.shape)\n",
    "print(len(word2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------\n",
    "# Model:\n",
    "## Define a basic LSTM-based Seq2Seq model using keras\n",
    "\n",
    "Instructions:\n",
    "\n",
    "You will be implementing the sequence-to-sequence model described in class where the model makes predictions\n",
    "using the left context and the dialogue context. More information model can be found in these lecture slides, this\n",
    "paper, or the reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "# Define an input sequence and process it.\n",
    "\n",
    "encoder_inputs = Input(shape=(mxlen,), name=\"Encoder_input\")\n",
    "\n",
    "embedding = Embedding(input_dim=len(word2id), \n",
    "                      output_dim=char_dim, \n",
    "                      input_length=mxlen, name=\"Embedding\")\n",
    "encoder_e = embedding(encoder_inputs)\n",
    "\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, name=\"Encoder_lstm\")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_e)\n",
    "#We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "\n",
    "# decoder, using `encoder_states` as initial state.\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_inputs = Input(shape=(mxlen,), name=\"Decoder_input\")\n",
    "\n",
    "decoder_e = embedding(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"Decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_e, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(len(word2id), activation='softmax', name=\"Dense_layer\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "my_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder_input (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_input (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Embedding)           (None, 20, 30)       1019490     Encoder_input[0][0]              \n",
      "                                                                 Decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_lstm (LSTM)             [(None, 256), (None, 293888      Embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_lstm (LSTM)             [(None, 20, 256), (N 293888      Embedding[1][0]                  \n",
      "                                                                 Encoder_lstm[0][1]               \n",
      "                                                                 Encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "Dense_layer (Dense)             (None, 20, 33983)    8733631     Decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,340,897\n",
      "Trainable params: 10,340,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "my_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(my_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 1296s 162ms/step - loss: 4.2033 - acc: 0.5251 - val_loss: 3.4223 - val_acc: 0.5104\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 1406s 176ms/step - loss: 3.1143 - acc: 0.5304 - val_loss: 3.4375 - val_acc: 0.5138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb32044f28>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.fit([input_tokens[:10000], output_tokens[:10000]], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer Decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder_lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'Encoder_lstm/while/Exit_4:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "my_model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Inference mode (sampling)\n",
    "Here we do the sampling to retrieve initial decoder state. <br>\n",
    "\n",
    "1) encode input and retrieve initial decoder state <br>\n",
    "2) run one step of decoder with this initial state and a \"start of sequence\" token as target. Output will be the next target token <br>\n",
    "3) Repeat with the current target token and current states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8ebda856ed30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's2s.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# encoder_inputs = model.input[0]   # input_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output   # lstm_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# encoder_states = [state_h_enc, state_c_enc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model('s2s.h5')\n",
    "\n",
    "# encoder_inputs = model.input[0]   # input_1\n",
    "# encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output   # lstm_1\n",
    "# encoder_states = [state_h_enc, state_c_enc]\n",
    "# encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# decoder_inputs = model.input[1]   # input_2\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,), name='input_3')\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,), name='input_4')\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# decoder_lstm = model.layers[3]\n",
    "# decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "#     decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# decoder_states = [state_h_dec, state_c_dec]\n",
    "# decoder_dense = model.layers[4]\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs] + decoder_states_inputs,\n",
    "#     [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling encoder models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()\n",
    "\n",
    "# Define sampling decoder models\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_e2 = embedding(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_e2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Predict decoder sequence\n",
    "At the last stage, we could predict the input sequence by putting in our predefined model. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, mxlen))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = word2id[\"<START>\"]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        outputs, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(outputs[0,-1,:])\n",
    "        sampled_token = id2word[sampled_token_index]\n",
    "        decoded_sentence += (sampled_token + \" \")\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_token == \"<EOS>\" or len(decoded_sentence) > mxlen):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, mxlen))\n",
    "        target_seq[0, 0] = word2id[\"<START>\"]\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------\n",
    "# Testing\n",
    "#### Here we can do the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [0 4 5 7 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 11 12  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 15  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 22  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 23 24 25 26 20 27 25 28 29 30 31 32 25 33 26 20  7 34  1]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 41 42 25  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 43 44 45  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0  8 46 47 48  7 41 44 45 49  1  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 61 30 62 63 64 23 35 65 66  1  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 43 67  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0  5 25 68  7 30 67  1  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 22  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 69  2 70 52 25 14 71 72 16 17 73 35 74 75 76  2  1  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 80  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 69 81 82 25 53  7 83  1  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 84 22  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 25 85 86  7 14 87 88 61 89 25  1  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0  8 90  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0  8 91 92 25 93 94 41 95 80 25 77 96  7 97 98  1  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 100 101 102   1   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  25  26 108   1   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 109  35 110 111  87 102 112   1   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 113   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  10  81  41 129  38 127  11 130 131 132   1   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 133  22   8 134 126 135  38 136 137 138  55  38 119 139  10  23 131\n",
      " 140 141]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 141   6 126   1   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0   8 151 151 151 152  14  80   8 153   6 154  36 155 156   1   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 140 100   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  20   5  25  48  59 166   7  73  35 121   1   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  16 167   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  25 100  36 168 169 126 170   1   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0   8 171  76  25   7 172  36 173  25  32 121 174 175 176 177 178 104\n",
      " 117 179]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  25 182 183  76  59 184   1   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 186   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  20 120 180 191 192  41 193 126 181 194 195   1   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  81 198 199 200 126 201  55   1   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0   8  33 202   7  26  20   7  83 121 203   8 202   7  26 204 110  35\n",
      " 185  41]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 214 215  16 216  92  41 217   1   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0   2 218   2  30 120  36 213   1   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 221  52 222 104 223 192 224 126 225   1   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  81 126 226   1   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 227 228  11 229   7  97 151 230 231  11 232 233 234  69 146  90  28\n",
      "  35  11]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 237   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  41 241 120 242  23  94  41 243  38 126 244 245 246  38 247  36 155\n",
      "   8 153]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 242   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  22  22 199  36 249 104 250 100 126 251 252   1   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 196 146   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  16 253  61  87  81  10 254 255  59 256 257   1   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  12  69  20 258 104 259  87 260 197 261 262 263   1   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0   6  41 264  32 265  32 266 267 268   1   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  84   8 269 270 271  88   2  52  81  12  88  25   1   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 220 104 272  30 273   2   2  32 274 275 175 276  74 277 278 279 280\n",
      " 281  76]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  25 189  25 283  41 222 284  94  41 285   1   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0   8 100   7  97 286 138 288 289   1   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  27   8 295  52  41 296 297 298   7 202   7  14  87  88 175  41 132\n",
      " 104 299]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 300   8  33 189  41 301  38 125 294 302 175 179   7 303   2  32   2\n",
      "   1   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 304   8  33  26   8 269 305  97 306  56  38 126 170   1   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 120 307 308 236 309   1   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 307 310 311 231 307 134  87 104  53  41 312 313   8 314 315   2 120\n",
      " 316  74]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 316   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 114   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 185  89 307  14 307  90  28 324   1   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 22  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  89  25 325  59 166   1   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  68   8 202   7 328   7  25  58  41 285   1   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 113 294   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 335 167 336   1   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 343   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 199 126 344 345 293  80 346  97  35  39 126 347  32  45   1   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 348 349   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  10 350 116 182  41 351 352 241 179  32  41 353 354 355  81  17  97\n",
      " 356  32]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 318  10  25 179   7 360   2 241  76 262   1   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 361   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 199  56   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 363 120 121 126   1   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 120 121 364 126 365   2 236 120 146  28  61   1   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  43   7 366 367  61   7 368  61   7 369 370  43   1   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0  8 28 86  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  25 371  61 281   1   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 219  14   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  25  91 372 373 263  25  26   1   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0   6  82 376 175  44 300  25 153  77 377  41 378  25 202   7   1   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  51  16  28  35 150  28 379  61 380 381 138  41 382  10   8 153 383\n",
      " 384  92]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0   8 374   8 269   8  90 385  25   1   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  23   6 386 240   7 387  59 388   1   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 81  6  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  22  25 250  52  25 151 269   8 223 272  36 389 390  25 391 219  61\n",
      "  14  87]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0   8  86   7 219  25 272 281  59 389 184  58 150   1   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 237 250  25 397  61   1   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 398 121   8 399 290  85   5 384  28 198 400 401  90 402 146  32   8\n",
      " 403 404]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 80  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  28 409 410   8 411 150   8 250 202   7 412   8 413 216 307 182 414\n",
      "  69 307]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [ 0 25 89 43  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 307 416 400  90 402 146  10   8  89 146   1   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 169 138   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 417   8   5  93  69  90 126 306  57   1   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0  80  25 418 294   1   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 307  90  35 126 419 420   1   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 237   1   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "-\n",
      "Input sentence: [  0 138 421  92 126 422   1   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3]\n",
      "Decoded sentence: <PAD> <PAD> <PAD> <PAD> \n"
     ]
    }
   ],
   "source": [
    "test_tokens = []\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = input_tokens[seq_index]\n",
    "    input_seq = input_seq.reshape(-1,input_seq.shape[0])\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_tokens[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
