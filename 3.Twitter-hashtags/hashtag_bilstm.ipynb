{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "mxlen=20\n",
    "\n",
    "char_map={\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5, \"f\": 6, \"g\": 7, \"h\": 8, \"i\": 9, \"j\": 10, \"k\": 11, \"l\": 12, \"m\": 13, \"n\": 14, \"o\": 15, \"p\": 16, \"q\": 17, \"r\": 18, \"s\": 19, \"t\": 20, \"u\": 21, \"v\": 22, \"w\": 23, \"x\": 24, \"y\": 25, \"z\": 26, \"A\": 27, \"B\": 28, \"C\": 29, \"D\": 30, \"E\": 31, \"F\": 32, \"G\": 33, \"H\": 34, \"I\": 35, \"J\": 36, \"K\": 37, \"L\": 38, \"M\": 39, \"N\": 40, \"O\": 41, \"P\": 42, \"Q\": 43, \"R\": 44, \"S\": 45, \"T\": 46, \"U\": 47, \"V\": 48, \"W\": 49, \"X\": 50, \"Y\": 51, \"Z\": 52, \"0\": 53, \"1\": 54, \"2\": 55, \"3\": 56, \"4\": 57, \"5\": 58, \"6\": 59, \"7\": 60, \"8\": 61, \"9\": 62, \"_\": 63, \"UNK\":64}\n",
    "\n",
    "\n",
    "def getTrainingData(hashtags, mxlen):\n",
    "    trainingData = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(hashtags)):\n",
    "        hashtag = hashtags[i]\n",
    "        train_hashtag = []\n",
    "        label = []\n",
    "        \n",
    "        for i in range(len(hashtag)-1):\n",
    "            letter = hashtag[i]\n",
    "            next_letter = hashtag[i+1]\n",
    "            if letter != \" \":\n",
    "                if next_letter == \" \":\n",
    "                    label.append(1)\n",
    "                else:\n",
    "                    label.append(0)\n",
    "                train_hashtag.append(letter)\n",
    "                \n",
    "        if hashtag != \" \":\n",
    "            train_hashtag.append(hashtag[-1])\n",
    "            label.append(0)\n",
    "            \n",
    "        labels.append(label)\n",
    "        trainingData.append(train_hashtag[:mxlen])\n",
    "\n",
    "    return trainingData, labels\n",
    "\n",
    "# pad input sequence to fixed length\n",
    "def pad(trainingData, labels, mxlen):\n",
    "    for i in range(len(trainingData)):\n",
    "        sample = trainingData[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        if len(sample) < mxlen:\n",
    "            sample += [-1] * (mxlen-len(sample)) \n",
    "            label += [-1] * (mxlen-len(label))\n",
    "\n",
    "        sample=np.array(sample[:mxlen])\n",
    "        label=np.array(label[:mxlen])\n",
    "\n",
    "    return np.array(trainingData), labels\n",
    "\n",
    "\n",
    "def get_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        hashtag_data = f.read().split('\\n')\n",
    "        hashtag_data = [h for h in hashtag_data if len(h) > 0]\n",
    "\n",
    "    originalData, labels = getTrainingData(hashtag_data, mxlen)\n",
    "    data, labels = pad(originalData, labels, mxlen)\n",
    "\n",
    "    samples = len(data)\n",
    "    data = np.asarray(data).reshape((samples*mxlen, 1))\n",
    "    new_trainingData=[]\n",
    "\n",
    "    for char in data:\n",
    "        cc=char[0]\n",
    "        if cc == \"-1\":\n",
    "            val=0\n",
    "        else:\n",
    "            if cc in char_map:\n",
    "                val=char_map[cc]\n",
    "            else:\n",
    "                val=char_map[\"UNK\"]\n",
    "\n",
    "        new_trainingData.append(val)\n",
    "\n",
    "    labels = np.asarray(labels).reshape((samples, mxlen, 1))\n",
    "    data = np.array(new_trainingData).reshape(samples, mxlen)\n",
    "\n",
    "    return originalData, data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "originalTraining, trainingData, labels = get_data(\"train.txt\")\n",
    "originalDev, devData, devLabels = get_data(\"dev.txt\")\n",
    "originalTest, testData, _ = get_data(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 20)            1300      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 20, 512)           567296    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 20, 1)             513       \n",
      "=================================================================\n",
      "Total params: 569,109\n",
      "Trainable params: 569,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = 256\n",
    "char_dim=20\n",
    "\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(char_map)+1, output_dim=char_dim, input_length=mxlen, mask_zero=True))\n",
    "#add Bidirectional LSTM layer here\n",
    "model.add(Bidirectional(LSTM(hidden_neurons, return_sequences=True),input_shape=(mxlen, char_dim)))\n",
    "#add Dense Time Distributed output layer here\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 705490 samples, validate on 1282 samples\n",
      "Epoch 1/2\n",
      "705490/705490 [==============================] - 1167s 2ms/step - loss: 0.1391 - acc: 0.9497 - val_loss: 0.1699 - val_acc: 0.9374\n",
      "Epoch 2/2\n",
      "705490/705490 [==============================] - 1092s 2ms/step - loss: 0.0704 - acc: 0.9740 - val_loss: 0.1377 - val_acc: 0.9530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb1f4a8d68>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE=256\n",
    "NUM_EPOCHS=2\n",
    "\n",
    "# Fit model on training data, evaluating on dev data\n",
    "model.fit(trainingData, labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=(devData, devLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "from keras.models import load_model\n",
    "model = load_model('model_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictions to Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to segmentation\n",
    "def segment(input_seq, ys):\n",
    "    \"\"\"\n",
    "    Return the original hashtag and the segmented hashtag\n",
    "       >>> input_seq = [g, o, b, e, a, r, s, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "       >>> ys = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ,0]\n",
    "       >>> segment(input_seq, ys)\n",
    "       gobears , go bears \n",
    "    \"\"\"\n",
    "    \n",
    "    original=[]\n",
    "    segmentation=[]\n",
    "    \n",
    "    ######################\n",
    "    ### YOUR CODE HERE ###\n",
    "    ######################\n",
    "    original = [i for i in input_seq if i !=-1]\n",
    "    segmentation = [i for i in input_seq if i !=-1]\n",
    "    ys = ys[:len(original)]\n",
    "    position = [i for i in range(len(ys)) if ys[i]]\n",
    "    for pos in reversed(position):\n",
    "        segmentation.insert(pos+1,' ')\n",
    "    \n",
    "    return original, segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g', 'o', 'b', 'e', 'a', 'r', 's']\n",
      "['g', 'o', ' ', 'b', 'e', 'a', 'r', 's']\n"
     ]
    }
   ],
   "source": [
    "#Test_Case 1\n",
    "input_seq = ['g', 'o', 'b', 'e', 'a', 'r', 's', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "ys = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ,0]\n",
    "original, segmentation = segment(input_seq, ys)\n",
    "print(original)\n",
    "print(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'h', 'i', 's', 'i', 's', 'a', 'b', 'i', 'r', 'd']\n",
      "['T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 'b', 'i', 'r', 'd']\n"
     ]
    }
   ],
   "source": [
    "#Test_Case 2\n",
    "input_seq = ['T', 'h', 'i', 's', 'i', 's', 'a', 'b', 'i', 'r', 'd', -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "ys = [0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "original, segmentation = segment(input_seq, ys)\n",
    "print(original)\n",
    "print(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill in above function segment before running this cell ###\n",
    "#load model\n",
    "from keras.models import load_model\n",
    "model = load_model('model_1.h5')\n",
    "\n",
    "# Generate predictions for test data written to output.txt\n",
    "out=open(\"output.txt\", \"w\")\n",
    "yhat = model.predict_classes(testData, verbose=0)\n",
    "idx=0\n",
    "samples,_,_ = yhat.shape\n",
    "for batch_num in range(samples):\n",
    "    vals=[]\n",
    "    for seq in range(mxlen):\n",
    "        vals.append(yhat[batch_num][seq][0])\n",
    "    original, segmentation=segment(originalTest[idx], vals)\n",
    "    out.write (\"%s\\t%s\\n\" % (''.join(original), ''.join(segmentation)))\n",
    "    idx+=1\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Chunking-System Evaluation F-1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for segmentation\n",
    "def segment_func(labels,cur):\n",
    "    chunks_ids = {}\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i]:\n",
    "            chunks_ids[i] = labels[cur:i+1]\n",
    "            cur = i+1\n",
    "        if i == len(labels)-1:\n",
    "            chunks_ids[i] = labels[cur:mxlen]\n",
    "    return chunks_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def segment_F1_score(pred_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Return average F1 score of segmentations provided by the model\n",
    "    \"\"\"\n",
    "    \n",
    "    ######################\n",
    "    ### YOUR CODE HERE ###\n",
    "    ######################\n",
    "    \n",
    "    f_score_list = []\n",
    "    for sen in range(len(true_labels)):\n",
    "        true_strim = [i[0] for i in true_labels[sen] if i !=-1]\n",
    "        pred_strim = [i[0] for i in pred_labels[sen][:len(true_strim)]]\n",
    "        \n",
    "        pred_chunks = segment_func(pred_strim,0)\n",
    "        true_chunks = segment_func(true_strim,0)\n",
    "    \n",
    "        correct = 0\n",
    "        for i in true_chunks:\n",
    "            if i in pred_chunks:\n",
    "                if true_chunks[i] == pred_chunks[i]:\n",
    "                    correct += 1\n",
    "        precision = correct/len(pred_chunks)\n",
    "        recall = correct/len(true_chunks)\n",
    "\n",
    "        f_score_list.append(2*precision*recall/(precision+recall+1e-18))\n",
    "            \n",
    "    \n",
    "    return sum(f_score_list)/len(f_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6250000000000001\n"
     ]
    }
   ],
   "source": [
    "#Test_Case 3\n",
    "ps = [[[0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0]]]\n",
    "ts = [[[0], [1], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0]]]\n",
    "print(segment_F1_score(ps, ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7251132902458948"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions for devData\n",
    "yhat = model.predict_classes(devData, verbose=0)\n",
    "segment_F1_score(yhat, devLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 [Extra Credit]  Kaggle & Architecture Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you changed the model architecture describe the changes you made here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate kaggle csv for test data\n",
    "kaggle_out=open(\"kaggle_output.csv\", \"w\")\n",
    "kaggle_out.write(\"id,expected\\n\")\n",
    "yhat = model.predict_classes(testData, verbose=0)\n",
    "idx=0\n",
    "samples,_,_ = yhat.shape\n",
    "for batch_num in range(samples):\n",
    "    vals=[]\n",
    "    for seq in range(mxlen):\n",
    "        vals.append(yhat[batch_num][seq][0])\n",
    "    original, segmentation=segment(originalTest[idx], vals)\n",
    "    kaggle_out.write (\"%s,%s\\n\" % (batch_num, ''.join(segmentation)))\n",
    "    idx+=1\n",
    "kaggle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
