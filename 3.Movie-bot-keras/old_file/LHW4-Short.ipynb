{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Chatbot\n",
    "## 2018.11.20\n",
    "Homework Instruction:\n",
    "\n",
    "In this homework you will be creating a Chatbot using a sequence-to-sequence model. You are allowed to work in\n",
    "groups of up to 2 students. This homework is of an open format; all we will be providing you with is the data. It is up\n",
    "to you to pre-process the data, build the seq-2-seq model with keras, and train the model. You will be submitting your\n",
    "code and write-up containing the three sections deﬁned below.\n",
    "\n",
    "<img src=\"https://cdn.technologyadvice.com/wp-content/uploads/2018/02/friendly-chatbot-700x408.jpg\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup:\n",
    "## Prepare data for neural network\n",
    "We'll begin by importing the needed models and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import LSTM, Embedding \n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Flatten, Reshape\n",
    "\n",
    "# hyperparameter\n",
    "mxlen = 10 # Max length for a sequence of tokens\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 3  # Number of epochs to train for.\n",
    "char_dim= 50 # Embedding size\n",
    "latent_dim = 50  # Latent dimensionality of the encoding space.\n",
    "\n",
    "word2id = {}\n",
    "id2word = {}\n",
    "\n",
    "data_path = \"movie_lines.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "        Load the movie_lines.tsv file which contains the data. \n",
    "        The ﬁle has ﬁve tab separated columns containing the following ﬁelds:\n",
    "        1. lineID\n",
    "        2. characterID (who uttered this phrase)\n",
    "        3. movieID\n",
    "        4. character name\n",
    "        5. text of the utterance\n",
    "        \n",
    "        all_lineids = [lineids1, lineids2, ...] where lineids is a sequence of \n",
    "            utterances for one movie.\n",
    "        all_ids = [ids1, ids2, ...] where ids is a sequence of \n",
    "            character ids for one movie.\n",
    "        all_toks = [toks1, toks2, ...] where toks is a sequence of \n",
    "            words (sentences) for one movie.\n",
    "    \n",
    "    \"\"\"\n",
    "    file = open(filename)\n",
    "    all_lineids = []\n",
    "    all_ids = []\n",
    "    all_toks = []\n",
    "    \n",
    "    lineids = []\n",
    "    ids = []\n",
    "    toks = []\n",
    "    mid = \"m0\"\n",
    "    i = 0\n",
    "    \n",
    "    for line in file:\n",
    "        cols = line.rstrip().split(\"\\t\")\n",
    "        #Only get the data with entire five columns\n",
    "        if len(cols) < 5:\n",
    "            continue\n",
    "        \n",
    "        if cols[2] != mid:\n",
    "            all_lineids.append(lineids)\n",
    "            all_ids.append(ids)\n",
    "            all_toks.append(toks)\n",
    "            \n",
    "            #Restart new movie data\n",
    "            lineids = [int(cols[0].strip('\"L'))]\n",
    "            ids = [int(cols[1].strip('u'))]\n",
    "            toks = [cols[4]]\n",
    "            mid = cols[2]\n",
    "            continue\n",
    "        \n",
    "        lineids.append(int(cols[0].strip('\"L')))\n",
    "        ids.append(int(cols[1].strip('u')))\n",
    "        toks.append(cols[4])\n",
    "        \n",
    "        i += 1\n",
    "        if i == 103085:\n",
    "            break\n",
    "    \n",
    "    if len(toks) > 0:\n",
    "        all_lineids.append(lineids)\n",
    "        all_ids.append(ids)\n",
    "        all_toks.append(toks)\n",
    "    \n",
    "    return all_lineids, all_ids, all_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lineids, all_ids, all_toks = load_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Text Prepare\n",
    "\n",
    "We are only providing you with the utterances. It is up to you to process the utterances into a format that can be fed\n",
    "into the model. <br>\n",
    "For information on how to process the data take a look at this paper: A Neural Conversation Model. <br>\n",
    "Some ideas for pre-processing may include removing infrequent words and replacing them with the <UNK> token.\n",
    "In your write-up include what you did to pre-process the dialogue data.\n",
    "    \n",
    "** 1) Remove bad symbols and tokenization <br> **\n",
    "** 2) *Count the words that appear only onces and replace them with <UNK\\> <br> **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Remove bad symbols and tokenization\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified string tokens \n",
    "                [tok1, tok2 , ...] which is a single sentence from one character\n",
    "    \"\"\"\n",
    "    tok = [\"<START>\"]\n",
    "    text = text.lower() # lowercase text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(BAD_SYMBOLS_RE, '', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    tok += (text.split()+[\"<EOS>\"])\n",
    "    \n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Dictionary of all words from train corpus with their counts.\n",
    "#    Dictionary of all words with its ids\n",
    "def count_words(all_toks):\n",
    "    \"\"\"\n",
    "    count:\n",
    "    { tok1: count1, tok2: count2, ...}\n",
    "    word2id:\n",
    "    { tok1: id1, tok2: id2, ...}\n",
    "    id2word:\n",
    "    { id1: tok1, id2: tok2, ...}\n",
    "    \n",
    "    \"\"\"\n",
    "    count = {}\n",
    "    \n",
    "    # Special Tokens\n",
    "    word2id[\"<START>\"] = 0\n",
    "    word2id[\"<EOS>\"] = 1\n",
    "    word2id[\"<UNK>\"] = 2\n",
    "    word2id[\"<PAD>\"] = 3\n",
    "    index = 4\n",
    "    \n",
    "    for toks in all_toks:\n",
    "        for tok in toks:\n",
    "            for word in tok:\n",
    "                # Count the words\n",
    "                if not word in count:\n",
    "                    count[word] = 1\n",
    "                else:\n",
    "                    count[word] += 1\n",
    "                # Make dictionary\n",
    "                if not word in word2id:\n",
    "                    word2id[word] = index\n",
    "                    index += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenize(all_toks):\n",
    "    \n",
    "    \"\"\"\n",
    "     all_toks_new: \n",
    "     [\n",
    "        movie 0:[ line 0: [tok1, tok2, ...],\n",
    "                  line 1: [tok1, tok2, ...],\n",
    "                  ... ]\n",
    "        movie 1:[ line 0: [tok1, tok2, ...],\n",
    "                  line 1: [tok1, tok2, ...],\n",
    "                  ... ]\n",
    "        movie 2:[ line 0: [tok1, tok2, ...],\n",
    "                  line 1: [tok1, tok2, ...],\n",
    "                  ... ]\n",
    "        ...\n",
    "\n",
    "     ]\n",
    "     \n",
    "     scarce_words_counts: a list of words that appear only once.\n",
    "     [tok1, tok2, tok3, ...]\n",
    "     \n",
    "    \"\"\"\n",
    "    all_toks_new = []\n",
    "\n",
    "    # Prepare the text\n",
    "    for toks in all_toks:\n",
    "        toks = [text_prepare(x) for x in toks]\n",
    "        all_toks_new.append(toks)\n",
    "\n",
    "    # Count the words that appears only once.\n",
    "    words_counts = count_words(all_toks_new)\n",
    "    scarce_words_counts = [x[0] for x in sorted(words_counts.items(), key = lambda x: x[1], reverse=True) if x[1] == 1]\n",
    "    \n",
    "    # Remove scarce words in word2id dictionary and reindex all words\n",
    "    for word in scarce_words_counts:\n",
    "        del word2id[word]\n",
    "    \n",
    "    return all_toks_new, scarce_words_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.03 s, sys: 36.5 ms, total: 1.07 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_toks_new, scarce_words_counts = text_tokenize(all_toks)\n",
    "word2id = {key: i for i, key in enumerate(word2id.keys())}\n",
    "id2word = {i:symbol for symbol, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration (per 100 movies):  0\n",
      "Iteration (per 100 movies):  1\n",
      "Iteration (per 100 movies):  2\n",
      "CPU times: user 6min 10s, sys: 1.49 s, total: 6min 11s\n",
      "Wall time: 6min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################################\n",
    "# Default: Skip, load the data directly\n",
    "# Replace the word with <UNK> that appears only once. This should take a while\n",
    "#for movie in all_toks_new:\n",
    "for i in range(len(all_toks_new)):\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration (per 100 movies): \",int(i/100))\n",
    "    for toks in all_toks_new[i]:\n",
    "        for j in range(len(toks)):\n",
    "            if toks[j] in scarce_words_counts:\n",
    "                toks[j] = \"<UNK>\"\n",
    "\n",
    "file1=open(\"all_toks_new.bin\",\"wb\")\n",
    "file2=open(\"word2id.bin\",\"wb\")\n",
    "pickle.dump(all_toks_new,file1)\n",
    "pickle.dump(word2id,file2)\n",
    "file1.close()\n",
    "file2.close()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data to save time\n",
    "file1=open(\"all_toks_new.bin\",\"rb\")\n",
    "file2=open(\"word2id.bin\",\"rb\")\n",
    "all_toks_new=pickle.load(file1)\n",
    "word2id=pickle.load(file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Vector and Padding \n",
    "In the following steps, we first make the word into id vectors which will be fed into keras embedding layers. <br>\n",
    "As for Padding: For the tokens into ids we have processed, we wish to pad all the sentences to a fixed mxlen, where we define 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(all_toks_new, mxlen):\n",
    "    \"\"\"\n",
    "    all_toks_new: (each with same length mxlen)\n",
    "     [\n",
    "        movie 0:[ line 0: [id1, id2, ...],\n",
    "                  line 1: [id1, id2, ...],\n",
    "                  ... ]\n",
    "        movie 1:[ line 0: [id1, id2, ...],\n",
    "                  line 1: [id1, id2, ...],\n",
    "                  ... ]\n",
    "        movie 2:[ line 0: [id1, id2, ...],\n",
    "                  line 1: [id1, id2, ...],\n",
    "                  ... ]\n",
    "        ...\n",
    "\n",
    "     ]\n",
    "    \"\"\"\n",
    "    for movie in all_toks_new:\n",
    "        for i in range(len(movie)):\n",
    "            #token to id\n",
    "            movie[i] = [word2id[x] for x in movie[i]]\n",
    "            #padding\n",
    "            if len(movie[i]) < mxlen:\n",
    "                movie[i] += [3] * (mxlen-len(movie[i])) \n",
    "            movie[i] = np.array(movie[i][:mxlen])\n",
    "    \n",
    "    return all_toks_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_toks_new = pad(all_toks_new, mxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Make encoding and decoding data\n",
    "Turn the data into three main data forms: <br>\n",
    "**input_tokens, output_tokens, output_target_tokens**\n",
    "\n",
    "\n",
    "for training a basic LSTM-based Seq2Seq model to predict output_target_tokens given input_tokens and output_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_conv(ids, toks):\n",
    "    \"\"\"\n",
    "    Separate the sequence of characters and their words if they utter continuously without waiting for the other to speak\n",
    "    For example:\n",
    "    ids = [2, 0, 2, 0, 2, 0, 0 ,2]\n",
    "    toks = [tok1, tok2, tok3, tok4, tok5, tok6, tok7, tok8]\n",
    "    sep_toks = [[tok1, tok2, tok3, tok4, tok5, tok6], [tok7, tok8]]\n",
    "    \n",
    "    \"\"\"\n",
    "    sep_toks = []\n",
    "    for i in range(len(ids)):\n",
    "        if i == 0:\n",
    "            temp = ids[i]\n",
    "            idx = i\n",
    "        else:\n",
    "            if temp == ids[i]:\n",
    "                sep_toks.append(toks[idx:i])\n",
    "                idx = i\n",
    "            temp = ids[i]\n",
    "        \n",
    "        if i == (len(ids)-1):\n",
    "            sep_toks.append(toks[idx:len(ids)])\n",
    "    \n",
    "    return sep_toks            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(all_lineids, all_ids, all_toks_new):\n",
    "    \"\"\"\n",
    "    Transform our original data with all dialogues all_toks_new into training data (input_tokens, output_tokens)\n",
    "    \n",
    "    A movie can be seen as an entity with sequential characters' conversations.\n",
    "    We deem a conversation end when two line ids are not consecutive.\n",
    "    for example, if a lineid sequence is [242, 241, 237, 236, 235]\n",
    "    we can make it into two conversations: [242, 241], [237, 236, 235]\n",
    "    \n",
    "    After specifying the conversations, we can then prepare the training data as follows:\n",
    "    for two conversations: [242, 241], [237, 236, 235] and corresponding token sequence is [toks1, toks2], [toks3, toks4, toks5]\n",
    "    we make input_tokens as [toks2], [toks4, toks5]\n",
    "            output_tokens as [toks1], [toks3, toks4]\n",
    "    \n",
    "    \n",
    "    Then we combine all tokens input-output pairs of every conversation in every movie.\n",
    "    so we will have\n",
    "    input_tokens = [toks2, toks4, toks5, toks7, ...]\n",
    "    output_tokens = [toks1, toks3, toks4, toks6, ...]\n",
    "    \n",
    "    Finally we wish to have our target output tokens to be almost same as output_tokens with each data ahead by one timestep.\n",
    "    output_target_tokens = [toks1, toks3, toks4, toks6, ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    input_tokens = []\n",
    "    output_tokens = []\n",
    "    \n",
    "    N = len(all_lineids) #number of movies\n",
    "\n",
    "    for i in range(N):\n",
    "        #For a single movie\n",
    "        movie = all_lineids[i]\n",
    "        for j in range(len(movie)):\n",
    "            if j == 0:\n",
    "                temp = movie[j]\n",
    "                idx = j\n",
    "            else:\n",
    "                if (temp-movie[j]) is not 1:\n",
    "                    sep_toks = separate_conv(all_ids[i][idx:j], all_toks_new[i][idx:j])\n",
    "                    for toks in sep_toks:\n",
    "                        input_tokens += toks[1:]\n",
    "                        output_tokens += toks[:-1]\n",
    "\n",
    "                    idx = j\n",
    "                temp = movie[j]\n",
    "\n",
    "            #Last Sequence\n",
    "            if j == len(movie)-1:\n",
    "                sep_toks = separate_conv(all_ids[i][idx:len(movie)], all_toks_new[i][idx:len(movie)])\n",
    "                for toks in sep_toks:\n",
    "                    input_tokens += toks[1:]\n",
    "                    output_tokens += toks[:-1]\n",
    "    \n",
    "    output_target_tokens = [0]*len(output_tokens)\n",
    "    for i in range(len(output_tokens)):\n",
    "        output_target_tokens[i] = np.concatenate((output_tokens[i][1:], [3]))\n",
    "            \n",
    "    return input_tokens, output_tokens, output_target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens, output_tokens, output_target_tokens = make_data(all_lineids, all_ids, all_toks_new)\n",
    "input_tokens = np.asarray(input_tokens)\n",
    "output_tokens = np.asarray(output_tokens)\n",
    "output_target_tokens = np.asarray(output_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 5 7 1 3 3 3 3 3]\n",
      "[0 4 5 6 1 3 3 3 3 3]\n",
      "[4 5 6 1 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(input_tokens[0])\n",
    "print(output_tokens[0])\n",
    "print(output_target_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = input_tokens[:1000]\n",
    "output_tokens = output_tokens[:1000]\n",
    "output_target_tokens = output_target_tokens[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data = np.zeros(\n",
    "    (len(output_target_tokens), mxlen, len(word2id)),\n",
    "    dtype='float32')\n",
    "for i, target in enumerate(output_target_tokens):\n",
    "    for t, idd in enumerate(target):\n",
    "        decoder_target_data[i, t, idd] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "(1000, 10)\n",
      "(1000, 10, 18223)\n",
      "18223\n"
     ]
    }
   ],
   "source": [
    "print(input_tokens.shape)\n",
    "print(output_target_tokens.shape)\n",
    "print(decoder_target_data.shape)\n",
    "print(len(word2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------\n",
    "# Model:\n",
    "## Define a basic LSTM-based Seq2Seq model using keras\n",
    "\n",
    "Instructions:\n",
    "\n",
    "You will be implementing the sequence-to-sequence model described in class where the model makes predictions\n",
    "using the left context and the dialogue context. More information model can be found in these lecture slides, this\n",
    "paper, or the reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "# Define an input sequence and process it.\n",
    "\n",
    "encoder_inputs = Input(shape=(mxlen,), name=\"Encoder_input\")\n",
    "\n",
    "embedding = Embedding(input_dim=len(word2id), \n",
    "                      output_dim=char_dim, \n",
    "                      input_length=mxlen, name=\"Embedding\")\n",
    "encoder_e = embedding(encoder_inputs)\n",
    "\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, name=\"Encoder_lstm\")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_e)\n",
    "#We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "\n",
    "# decoder, using `encoder_states` as initial state.\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_inputs = Input(shape=(mxlen,), name=\"Decoder_input\")\n",
    "\n",
    "decoder_e = embedding(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"Decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_e, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(len(word2id), activation='softmax', name=\"Dense_layer\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "my_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder_input (InputLayer)      (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_input (InputLayer)      (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Embedding)           (None, 10, 50)       911150      Encoder_input[0][0]              \n",
      "                                                                 Decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_lstm (LSTM)             [(None, 50), (None,  20200       Embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_lstm (LSTM)             [(None, 10, 50), (No 20200       Embedding[1][0]                  \n",
      "                                                                 Encoder_lstm[0][1]               \n",
      "                                                                 Encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "Dense_layer (Dense)             (None, 10, 18223)    929373      Decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,880,923\n",
      "Trainable params: 1,880,923\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "my_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(my_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/3\n",
      "800/800 [==============================] - 30s 38ms/step - loss: 8.8172 - acc: 0.2728 - val_loss: 8.7932 - val_acc: 0.3345\n",
      "Epoch 2/3\n",
      "800/800 [==============================] - 29s 36ms/step - loss: 8.7123 - acc: 0.3381 - val_loss: 8.4846 - val_acc: 0.3345\n",
      "Epoch 3/3\n",
      "800/800 [==============================] - 28s 35ms/step - loss: 7.8778 - acc: 0.3385 - val_loss: 7.1652 - val_acc: 0.3365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer Decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder_lstm/while/Exit_3:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'Encoder_lstm/while/Exit_4:0' shape=(?, 50) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "my_model.fit([input_tokens, output_tokens], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "############################################\n",
    "#################Optional###################\n",
    "############################################\n",
    "# Save model\n",
    "my_model.save('s2s.h5')\n",
    "############################################\n",
    "#################Optional###################\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#################Optional###################\n",
    "############################################\n",
    "model = load_model('s2s.h5')\n",
    "\n",
    "encoder_inputs = model.input[0]   # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output   # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]   # input_2\n",
    "decoder_state_input_h = Input(shape=(latent_dim,), name='input_3')\n",
    "decoder_state_input_c = Input(shape=(latent_dim,), name='input_4')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "############################################\n",
    "#################Optional###################\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Inference mode (sampling)\n",
    "Here we do the sampling to retrieve initial decoder state. <br>\n",
    "\n",
    "1) encode input and retrieve initial decoder state <br>\n",
    "2) run one step of decoder with this initial state and a \"start of sequence\" token as target. Output will be the next target token <br>\n",
    "3) Repeat with the current target token and current states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder_input (InputLayer)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "Embedding (Embedding)        (None, 10, 50)            911150    \n",
      "_________________________________________________________________\n",
      "Encoder_lstm (LSTM)          [(None, 50), (None, 50),  20200     \n",
      "=================================================================\n",
      "Total params: 931,350\n",
      "Trainable params: 931,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define sampling encoder models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling decoder models\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_e2 = embedding(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_e2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Predict decoder sequence\n",
    "At the last stage, we could predict the input sequence by putting in our predefined model. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, mxlen))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = word2id[\"<START>\"]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        outputs, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(outputs[0,-1,:])\n",
    "        sampled_token = id2word[sampled_token_index]\n",
    "        decoded_sentence += (sampled_token + \" \")\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_token == \"<EOS>\" or len(decoded_sentence) > mxlen):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, mxlen))\n",
    "        target_seq[0, 0] = word2id[\"<START>\"]\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------\n",
    "# Testing\n",
    "#### Here we can do the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: [0 4 5 7 1 3 3 3 3 3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 11 12  1  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 15  1  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 22  1  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 23 24 25 26 20 27 25 28 29]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 41 42 25  1  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 43 44 45  1  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0  8 46 47 48  7 41 44 45 49]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 60 30 61 62 63 23 35 64 65]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 43 66  1  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0  5 25 67  7 30 66  1  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 22  1  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 68  2 69 52 25 14 70 71 16]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 79  1  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 68 80 81 25 53  7 82  1  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 83 22  1  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 25 84 85  7 14 86 87 60 88]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0  8 89  1  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0  8 90 91 25 92 93 41 94 79]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  99 100 101   1   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  25  26 107   1   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 108  35 109 110  86 101 111   1   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 112   1   3   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  10  80  41 128  38 126  11 129 130]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 132  22   8 133 125 134  38   2   2]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 138   6 125   1   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0   8 148 148 148 149  14  79   8 150]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 137  99   1   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  20   5  25  48  59 161   7  72  35]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  16 162   1   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  25  99  36 163 164 125 165   1   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0   8 166  75  25   7 167  36 168  25]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  25 177 178  75  59 179   1   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 181   1   3   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  20 119 175 186 187  41 188 125 176]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  80 193 194 195 125 196  55   1   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0   8  33 197   7  26  20   7  82 120]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 209 210  16 211  91  41 212   1   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0   2 213   2  30 119  36 208   1   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 216  52 217 103 218 187 219 125 220]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  80 125 221   1   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 222 223  11 224   7  96 148 225 226]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 232   1   3   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  41 236 119 237  23  93  41 238  38]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 237   1   3   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  22  22 194  36 244 103 245  99 125]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 191 143   1   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  16 248  60  86  80  10 249 250  59]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  12  68  20 253 103 254  86 255 192]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0   6  41 259  32 260  32 261 262 263]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  83   8 264 265 266  87   2  52  80]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 215 103 267  30 268   2   2  32 269]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  25 184  25 277  41 217 278  93  41]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0   8  99   7  96 280 135 282 283   1]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  27   8 289  52  41 290 291 292   7]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 294   8  33 184  41 295  38 124 288]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 298   8  33  26   8 264 299  96 300]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 119 301 302 231 303   1   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 301 304 305 226 301 133  86 103  53]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 310   1   3   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 113   1   3   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 180  88 301  14 301  89  28 318   1]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 22  1  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  88  25 319  59 161   1   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  67   8 197   7 322   7  25  58  41]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 112 288   1   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 329 162 330   1   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 336   1   3   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 194 125 337 338 287  79 339  96  35]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 341 342   1   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  10 343 115 177  41 344   2 236 174]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 312  10  25 174   7 352   2 236  75]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 353   1   3   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 194  56   1   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 355 119 120 125   1   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 119 120 356 125 357   2 231 119 143]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  43   7 358 359  60   7 360  60   7]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0  8 28 85  1  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  25 363  60 275   1   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 214  14   1   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  25  90 364 365 258  25  26   1   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0   6  81 368 170  44 294  25 150  76]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  51  16  28  35 147  28 371  60 372]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0   8 366   8 264   8  89 377  25   1]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  23   6 378 235   7 379  59 380   1]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 80  6  1  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  22  25 245  52  25 148 264   8 218]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0   8  85   7 214  25 267 275  59 381]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 232 245  25 389  60   1   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 390 120   8 391 284  84   5 376  28]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 79  1  3  3  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  28 400 401   8 402 147   8 245 197]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [ 0 25 88 43  1  3  3  3  3  3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 301 407 392  89 394 143  10   8  88]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 164 135   1   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 408   8   5  92  68  89 125 300  57]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0  79  25 409 288   1   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 301  89  35 125 410 411   1   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: [  0 232   1   3   3   3   3   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n",
      "-\n",
      "Input sentence: [  0 135 412  91 125 413   1   3   3   3]\n",
      "Decoded sentence: <PAD> <PAD> \n"
     ]
    }
   ],
   "source": [
    "test_tokens = []\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = input_tokens[seq_index]\n",
    "    input_seq = input_seq.reshape(-1,input_seq.shape[0])\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_tokens[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
