{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Chatbot\n",
    "## 2018.11.20\n",
    "Homework Instruction:\n",
    "\n",
    "In this homework you will be creating a Chatbot using a sequence-to-sequence model. You are allowed to work in\n",
    "groups of up to 2 students. This homework is of an open format; all we will be providing you with is the data. It is up\n",
    "to you to pre-process the data, build the seq-2-seq model with keras, and train the model. You will be submitting your\n",
    "code and write-up containing the three sections deﬁned below.\n",
    "\n",
    "<img src=\"https://cdn.technologyadvice.com/wp-content/uploads/2018/02/friendly-chatbot-700x408.jpg\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup:\n",
    "## Prepare data for neural network\n",
    "We'll begin by importing the needed models and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:07:01.850423Z",
     "start_time": "2018-12-02T14:07:00.292524Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import LSTM, Embedding \n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Flatten, Reshape\n",
    "from keras import optimizers\n",
    "\n",
    "# hyperparameter\n",
    "mxlen = 20 # Max length for a sequence of tokens\n",
    "batch_size = 128  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "char_dim= 50 # Embedding size\n",
    "latent_dim = 50  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# dictionary\n",
    "word2id = {} # Count all word library\n",
    "id2word = {} # Reverse word2id\n",
    "input_word2id = {} # Count input word library\n",
    "output_word2id = {} # Count output word library\n",
    "input_id2word = {} # Reverse input_word2id\n",
    "output_id2word = {} # Reverse output_word2id\n",
    "\n",
    "data_path = \"data/movie_lines.tsv\"\n",
    "conversation_path = \"data/movie_conversations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Remove bad symbols and tokenization\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified string tokens \n",
    "                [tok1, tok2 , ...] which is a single sentence from one character\n",
    "    \"\"\"\n",
    "    tok = [\"<START>\"] # add START token to represent sentence start\n",
    "    text = text.lower() # lowercase text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(BAD_SYMBOLS_RE, '', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    tok += (text.split()+[\"<EOS>\"]) # add EOS token to represent sentence end\n",
    "    \n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Dictionary of all words from train corpus with their counts.\n",
    "#    Dictionary of all words with its ids\n",
    "def count_words(line_dict, word2id, id2word):\n",
    "    \"\"\"\n",
    "    count:\n",
    "    { tok1: count1, tok2: count2, ...}\n",
    "    word2id:\n",
    "    { tok1: id1, tok2: id2, ...}\n",
    "    id2word:\n",
    "    { id1: tok1, id2: tok2, ...}\n",
    "    \n",
    "    \"\"\"\n",
    "    count = {}\n",
    "    \n",
    "    # Special Tokens\n",
    "    word2id[\"<START>\"] = 0\n",
    "    word2id[\"<EOS>\"] = 1\n",
    "    word2id[\"<UNK>\"] = 2\n",
    "    index = 3\n",
    "    \n",
    "    for toks in line_dict.values():\n",
    "        for word in toks:\n",
    "            # Count the words\n",
    "            if not word in count:\n",
    "                count[word] = 1\n",
    "            else:\n",
    "                count[word] += 1\n",
    "            # Make dictionary\n",
    "            if not word in word2id:\n",
    "                word2id[word] = index\n",
    "                index += 1\n",
    "    \n",
    "    # Count the words that appears only once.\n",
    "    scarce_words_counts = [x[0] for x in sorted(count.items(), key = lambda x: x[1], reverse=True) if x[1] == 1]\n",
    "    \n",
    "    # Remove scarce words in word2id dictionary and reindex all words\n",
    "    for word in scarce_words_counts:\n",
    "        del word2id[word]\n",
    "    \n",
    "    # Arrange word2id and id2word\n",
    "    word2id = {key: i for i, key in enumerate(word2id.keys())}\n",
    "    id2word = {i:symbol for symbol, i in word2id.items()}\n",
    "    \n",
    "    return scarce_words_counts, word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:07:03.848199Z",
     "start_time": "2018-12-02T14:07:03.756068Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_text(data_path, word2id, id2word):\n",
    "    \"\"\"\n",
    "        Load the movie_lines.tsv file which contains the data. \n",
    "        The ﬁle has ﬁve tab separated columns containing the following ﬁelds:\n",
    "        1. lineID\n",
    "        2. characterID (who uttered this phrase)\n",
    "        3. movieID\n",
    "        4. character name\n",
    "        5. text of the utterance\n",
    "        \n",
    "        Here we only extract lineID and utterance\n",
    "        \n",
    "        line_dict = {lineID1: utterance1,\n",
    "                     lineID2: utterance2,\n",
    "                     lineID3: utterance3,\n",
    "                     ...}\n",
    "    \"\"\"\n",
    "    file1 = open(data_path)\n",
    "    line_dict = {}\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for line in file1:\n",
    "        cols = line.rstrip().split(\"\\t\")\n",
    "        line_dict[cols[0].replace('\"','')] = text_prepare(cols[-1])\n",
    "        \n",
    "        i += 1\n",
    "        # 400 movies\n",
    "        if i == 308298:\n",
    "            break\n",
    "    \n",
    "    scarce_words_counts, word2id, id2word = count_words(line_dict, word2id, id2word)\n",
    "    \n",
    "    return line_dict, scarce_words_counts, word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conversations(conversation_path, line_dict, scarce_words_counts, prev_sent=2):\n",
    "    \"\"\"\n",
    "        Load movie_conversations.txt which has the conversation lists\n",
    "        all_convs = [converation 1: [('', sent1, sent2),\n",
    "                                     (sent1, sent2, sent3),\n",
    "                                     (sent2, sent3, sent4),\n",
    "                                     ...]]\n",
    "        num_data: number of all data\n",
    "        num_conv: number of all conversation pairs\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file2 = open(conversation_path, 'r').read().split('\\n')[:-1]\n",
    "    all_convs = []\n",
    "    num_data, num_conv = 0, 0\n",
    "    for i,conv in enumerate(file2):\n",
    "        convs = []\n",
    "        DELETE = re.compile('[/(){}\\[\\]\\|@,;\\']')\n",
    "        conv = re.sub(DELETE, '', conv.split(' +++$+++ ')[-1]).split()\n",
    "        \n",
    "        con_a_1 = []\n",
    "        for i in range(len(conv)-1):\n",
    "            con_a_2 = [line_dict[conv[i]]\n",
    "            con_b = line_dict[conv[i+1]]\n",
    "            convs.append((con_a_1+con_a_2, con_b) if prev_sent==2 else (con_a_2, con_b))\n",
    "            num_data += 1\n",
    "            con_a_1 = con_a_2\n",
    "            \n",
    "        num_conv += 1\n",
    "        all_convs.append(convs)\n",
    "    \n",
    "    return all_convs, num_data, num_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:07:05.728637Z",
     "start_time": "2018-12-02T14:07:05.364534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.5 s, sys: 173 ms, total: 5.67 s\n",
      "Wall time: 5.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "line_dict, scarce_words_counts, word2id, id2word = load_text(data_path, word2id, id2word)\n",
    "all_convs, num_data, num_conv = load_conversations(line_dict, conversation_path, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<START>',\n",
       "  'well',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'wed',\n",
       "  'start',\n",
       "  'with',\n",
       "  'pronunciation',\n",
       "  'if',\n",
       "  'thats',\n",
       "  'okay',\n",
       "  'with',\n",
       "  'you',\n",
       "  '<EOS>'],\n",
       " ['<START>',\n",
       "  'not',\n",
       "  'the',\n",
       "  'hacking',\n",
       "  'and',\n",
       "  'gagging',\n",
       "  'and',\n",
       "  'spitting',\n",
       "  'part',\n",
       "  'please',\n",
       "  '<EOS>'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_convs[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Replace and restrict Word length\n",
    "In the following steps, we will replace the word that only appears once with <UNK> token and restrict sentence to mxlen length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:07:14.294137Z",
     "start_time": "2018-12-02T14:07:14.265840Z"
    }
   },
   "outputs": [],
   "source": [
    "def modify(all_toks_new, scarce_words_counts, mxlen):\n",
    "    \"\"\"\n",
    "    all_toks_new: (each with same length mxlen)\n",
    "     [\n",
    "        movie 0:[ line 0: [id1, id2, ...],\n",
    "                  line 1: [id1, id2, ...],\n",
    "                  ... ]\n",
    "        movie 1:[ line 0: [id1, id2, ...],\n",
    "                  line 1: [id1, id2, ...],\n",
    "                  ... ]\n",
    "        movie 2:[ line 0: [id1, id2, ...],\n",
    "                  line 1: [id1, id2, ...],\n",
    "                  ... ]\n",
    "        ...\n",
    "\n",
    "     ]\n",
    "     \n",
    "    scarce_words_counts: A list with words that only appear once\n",
    "    [ token1, token2, token3, ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    # Replace the word with <UNK> that appears only once.\n",
    "    # for movie in all_toks_new:\n",
    "    for i in range(len(all_toks_new)):\n",
    "        if i % 100 == 0:\n",
    "            print(\"Iteration (per 100 movies): \",int(i/100))\n",
    "        for toks in all_toks_new[i]:\n",
    "            for j in range(len(toks)):\n",
    "                if toks[j] in scarce_words_counts:\n",
    "                    toks[j] = \"<UNK>\"\n",
    "    \n",
    "    # Cut the sentence to mxlen\n",
    "    for movie in all_toks_new:\n",
    "        for i in range(len(movie)):\n",
    "            movie[i] = np.array(movie[i][:mxlen])\n",
    "    \n",
    "    return all_toks_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration (per 100 movies):  0\n",
      "Iteration (per 100 movies):  1\n",
      "Iteration (per 100 movies):  2\n",
      "CPU times: user 7min 35s, sys: 2.74 s, total: 7min 38s\n",
      "Wall time: 8min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################################\n",
    "# Default: Skip, load the data directly\n",
    "all_toks_new = modify(all_toks_new, scarce_words_counts, mxlen)\n",
    "file1=open(\"all_toks_new.bin\",\"wb\")\n",
    "file2=open(\"word2id.bin\",\"wb\")\n",
    "pickle.dump(all_toks_new,file1)\n",
    "pickle.dump(word2id,file2)\n",
    "file1.close()\n",
    "file2.close()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:07:18.510003Z",
     "start_time": "2018-12-02T14:07:17.880318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the processed data to save time\n",
    "file1=open(\"all_toks_new.bin\",\"rb\")\n",
    "file2=open(\"word2id.bin\",\"rb\")\n",
    "all_toks_new=pickle.load(file1)\n",
    "word2id=pickle.load(file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Make encoding and decoding data\n",
    "1) Turn the data into two main data forms: <br>\n",
    "   **input_tokens, output_tokens**<br>\n",
    "2) Then we will have to prepare two word dictionary **input_word2id, output_word2id** to turn word into ids (bag-of-word) <br>\n",
    "3) Calculate their length as **num_encoder_tokens, num_decoder_tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First part: \n",
    "Turn into input and output tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:07:24.317367Z",
     "start_time": "2018-12-02T14:07:24.295224Z"
    }
   },
   "outputs": [],
   "source": [
    "def separate_conv(ids, toks):\n",
    "    \"\"\"\n",
    "    Separate the sequence of characters and their words if they utter continuously without waiting for the other to speak\n",
    "    For example:\n",
    "    ids = [2, 0, 2, 0, 2, 0, 0 ,2]\n",
    "    toks = [tok1, tok2, tok3, tok4, tok5, tok6, tok7, tok8]\n",
    "    sep_toks = [[tok1, tok2, tok3, tok4, tok5, tok6], [tok7, tok8]]\n",
    "    \n",
    "    \"\"\"\n",
    "    sep_toks = []\n",
    "    for i in range(len(ids)):\n",
    "        if i == 0:\n",
    "            temp = ids[i]\n",
    "            idx = i\n",
    "        else:\n",
    "            if temp == ids[i]:\n",
    "                sep_toks.append(toks[idx:i])\n",
    "                idx = i\n",
    "            temp = ids[i]\n",
    "        \n",
    "        if i == (len(ids)-1):\n",
    "            sep_toks.append(toks[idx:len(ids)])\n",
    "    \n",
    "    return sep_toks            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:07:30.776265Z",
     "start_time": "2018-12-02T14:07:30.717116Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_data(all_lineids, all_ids, all_toks_new):\n",
    "    \"\"\"\n",
    "    Transform our original data with all dialogues all_toks_new into training data (input_tokens, output_tokens)\n",
    "    \n",
    "    A movie can be seen as an entity with sequential characters' conversations.\n",
    "    We deem a conversation end when two line ids are not consecutive.\n",
    "    for example, if a lineid sequence is [242, 241, 237, 236, 235]\n",
    "    we can make it into two conversations: [242, 241], [237, 236, 235]\n",
    "    \n",
    "    After specifying the conversations, we can then prepare the training data as follows:\n",
    "    for two conversations: [242, 241], [237, 236, 235] and corresponding token sequence is [toks1, toks2], [toks3, toks4, toks5]\n",
    "    we make input_tokens as [toks2], [toks4, toks5]\n",
    "            output_tokens as [toks1], [toks3, toks4]\n",
    "    \n",
    "    \n",
    "    Then we combine all tokens input-output pairs of every conversation in every movie.\n",
    "    so we will have\n",
    "    input_tokens = [toks2, toks4, toks5, toks7, ...]\n",
    "    output_tokens = [toks1, toks3, toks4, toks6, ...]\n",
    "    \n",
    "    Finally we wish to have our target output tokens to be almost same as output_tokens with each data ahead by one timestep.\n",
    "    output_target_tokens = [toks1, toks3, toks4, toks6, ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    input_tokens = []\n",
    "    output_tokens = []\n",
    "    \n",
    "    N = len(all_lineids) #number of movies\n",
    "\n",
    "    for i in range(N):\n",
    "        #For a single movie\n",
    "        movie = all_lineids[i]\n",
    "        for j in range(len(movie)):\n",
    "            if j == 0:\n",
    "                temp = movie[j]\n",
    "                idx = j\n",
    "            else:\n",
    "                if (temp-movie[j]) is not 1:\n",
    "                    sep_toks = separate_conv(all_ids[i][idx:j], all_toks_new[i][idx:j])\n",
    "                    for toks in sep_toks:\n",
    "                        input_tokens += toks[1:]\n",
    "                        output_tokens += toks[:-1]\n",
    "\n",
    "                    idx = j\n",
    "                temp = movie[j]\n",
    "\n",
    "            #Last Sequence\n",
    "            if j == len(movie)-1:\n",
    "                sep_toks = separate_conv(all_ids[i][idx:len(movie)], all_toks_new[i][idx:len(movie)])\n",
    "                for toks in sep_toks:\n",
    "                    input_tokens += toks[1:]\n",
    "                    output_tokens += toks[:-1]\n",
    "            \n",
    "    return input_tokens, output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:07:36.937218Z",
     "start_time": "2018-12-02T14:07:36.803876Z"
    }
   },
   "outputs": [],
   "source": [
    "input_tokens, output_tokens = make_data(all_lineids, all_ids, all_toks_new)\n",
    "input_tokens = np.asarray(input_tokens)\n",
    "output_tokens = np.asarray(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:07:43.191980Z",
     "start_time": "2018-12-02T14:07:43.179719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>' 'tell' 'you' 'what' 'lets' 'ditch' 'the' 'limo' 'let' 'me'\n",
      " 'drive' 'you' 'up' 'to' 'that' 'red' 'carpet' 'in' 'my' 'beat']\n",
      "['<START>' 'the' 'hell' 'you' 'will' 'harry' 'york' '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(input_tokens[23444])\n",
    "print(output_tokens[23444])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second part: \n",
    "Turn into encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:07:49.329830Z",
     "start_time": "2018-12-02T14:07:49.316620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79590\n"
     ]
    }
   ],
   "source": [
    "print(len(input_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:07:55.673970Z",
     "start_time": "2018-12-02T14:07:55.649481Z"
    }
   },
   "outputs": [],
   "source": [
    "input_tokens = input_tokens[:30000]\n",
    "output_tokens = output_tokens[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:08:08.850990Z",
     "start_time": "2018-12-02T14:08:01.577967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "all_input_words = set()\n",
    "all_output_words = set()\n",
    "\n",
    "# Calculate input words and output words as a sorted list\n",
    "for toks in input_tokens:\n",
    "    for tok in toks:\n",
    "        if tok not in all_input_words:\n",
    "            all_input_words.add(tok)\n",
    "for toks in output_tokens:\n",
    "    for tok in toks:\n",
    "        if tok not in all_output_words:\n",
    "            all_output_words.add(tok)\n",
    "all_input_words = sorted(list(all_input_words))\n",
    "all_output_words = sorted(list(all_output_words))\n",
    "\n",
    "# Make input and output libraries\n",
    "num_encoder_tokens = len(all_input_words)\n",
    "num_decoder_tokens = len(all_output_words)\n",
    "input_word2id = dict([(word, i) for i, word in enumerate(all_input_words)])\n",
    "output_word2id = dict([(word, i) for i, word in enumerate(all_output_words)])\n",
    "input_id2word = dict((i, tok) for tok, i in input_word2id.items())\n",
    "output_id2word = dict((i, tok) for tok, i in output_word2id.items())\n",
    "\n",
    "# Make encoder_input_data, decoder_input_data, decoder_target_data\n",
    "encoder_input_data = np.zeros((len(input_tokens), mxlen), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(output_tokens), mxlen), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(output_tokens), mxlen, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "for i, (input_text, output_text) in enumerate(zip(input_tokens, output_tokens)):\n",
    "    for t, word in enumerate(input_text):\n",
    "        encoder_input_data[i, t] = input_word2id[word]\n",
    "    for t, word in enumerate(output_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = output_word2id[word]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, output_word2id[word]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------\n",
    "# Model:\n",
    "## Define a basic LSTM-based Seq2Seq model using keras\n",
    "\n",
    "Instructions:\n",
    "\n",
    "You will be implementing the sequence-to-sequence model described in class where the model makes predictions\n",
    "using the left context and the dialogue context. More information model can be found in these lecture slides, this\n",
    "paper, or the reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:08:17.969305Z",
     "start_time": "2018-12-02T14:08:16.531442Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,), name=\"Encoder_input\")\n",
    "\n",
    "encoder_embedding = Embedding(input_dim=num_encoder_tokens, \n",
    "                              output_dim=char_dim, name=\"Encoder_Embedding\")\n",
    "encoder_e = encoder_embedding(encoder_inputs)\n",
    "\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, name=\"Encoder_lstm\")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_e)\n",
    "#We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "\n",
    "# decoder, using `encoder_states` as initial state.\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_inputs = Input(shape=(None,), name=\"Decoder_input\")\n",
    "\n",
    "decoder_embedding = Embedding(input_dim=num_decoder_tokens, \n",
    "                              output_dim=char_dim, name=\"Decoder_Embedding\")\n",
    "decoder_e = decoder_embedding(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"Decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_e, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name=\"Dense_layer\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "my_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T14:08:24.003662Z",
     "start_time": "2018-12-02T14:08:23.942992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_Embedding (Embedding)   (None, None, 50)     667500      Encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Embedding (Embedding)   (None, None, 50)     673050      Decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_lstm (LSTM)             [(None, 50), (None,  20200       Encoder_Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_lstm (LSTM)             [(None, None, 50), ( 20200       Decoder_Embedding[0][0]          \n",
      "                                                                 Encoder_lstm[0][1]               \n",
      "                                                                 Encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "Dense_layer (Dense)             (None, None, 13461)  686511      Decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,067,461\n",
      "Trainable params: 2,067,461\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "optimizer = optimizers.RMSprop(lr=0.002, rho=0.9, epsilon=None, decay=0.0)\n",
    "my_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(my_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-02T14:08:26.123Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33250 samples, validate on 1750 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "my_model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "############################################\n",
    "#################Optional###################\n",
    "############################################\n",
    "# Save model\n",
    "my_model.save('s2s.h5')\n",
    "############################################\n",
    "#################Optional###################\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Inference mode (sampling)\n",
    "Here we do the sampling to retrieve initial decoder state. <br>\n",
    "\n",
    "1) encode input and retrieve initial decoder state <br>\n",
    "2) run one step of decoder with this initial state and a \"start of sequence\" token as target. Output will be the next target token <br>\n",
    "3) Repeat with the current target token and current states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder_input (InputLayer)   (None, None)              0         \n",
      "_________________________________________________________________\n",
      "Encoder_Embedding (Embedding (None, None, 50)          468800    \n",
      "_________________________________________________________________\n",
      "Encoder_lstm (LSTM)          [(None, 50), (None, 50),  20200     \n",
      "=================================================================\n",
      "Total params: 489,000\n",
      "Trainable params: 489,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define sampling encoder models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling decoder models\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_e2 = decoder_embedding(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_e2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Predict decoder sequence\n",
    "At the last stage, we could predict the input sequence by putting in our predefined model. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "e_model = load_model(\"encoder.h5\")\n",
    "d_model = load_model(\"decoder.h5\")\n",
    "\n",
    "def decode_sequence(input_seq,e_model,d_model):\n",
    "    \n",
    "    # Encode the input as state vectors.\n",
    "    states_value = e_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = output_word2id['<START>']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        outputs, h, c = d_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(outputs[0, -1, :])\n",
    "        sampled_tok = output_id2word[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_tok\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_tok == '<EOS>'):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    decoded_sentence = decoded_sentence.strip('<EOS>')\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------\n",
    "# Testing\n",
    "#### Here we can do the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_seq(data):\n",
    "    token_data = [text_prepare(text) for text in data]\n",
    "    encoder_data = np.zeros((len(token_data), mxlen), dtype='float32')\n",
    "\n",
    "    for i, input_text in enumerate(token_data):\n",
    "        for t, word in enumerate(input_text):\n",
    "            if word in input_word2id:\n",
    "                encoder_data[i, t] = input_word2id[word]\n",
    "            else:\n",
    "                encoder_data[i, t] = 2\n",
    "    return encoder_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: my name is david, what is my name?\n",
      "Decoded sentence:  oh no i dont know it \n",
      "-\n",
      "Input sentence: my name is john, what is my name?\n",
      "Decoded sentence:  <UNK> \n",
      "-\n",
      "Input sentence: are you a leader or a follower?\n",
      "Decoded sentence:  what do you want me to say \n",
      "-\n",
      "Input sentence: are you a follower or a leader?\n",
      "Decoded sentence:  hey what do you mean \n",
      "-\n",
      "Input sentence: what is moral?\n",
      "Decoded sentence:  <UNK> <UNK> <UNK> \n",
      "-\n",
      "Input sentence: what is immoral?\n",
      "Decoded sentence:  the <UNK> \n",
      "-\n",
      "Input sentence: what is altruism?\n",
      "Decoded sentence:  the <UNK> \n",
      "-\n",
      "Input sentence: ok ... so what is the deﬁnition of morality?\n",
      "Decoded sentence:  the world is the only thing that would never have live to the same \n",
      "-\n",
      "Input sentence: tell me the deﬁnition of morality , i am quite upset now!\n",
      "Decoded sentence:  what the fuck are you doing here \n"
     ]
    }
   ],
   "source": [
    "# Test input data\n",
    "data = [\"my name is david, what is my name?\",\n",
    "        \"my name is john, what is my name?\",\n",
    "        \"are you a leader or a follower?\",\n",
    "        \"are you a follower or a leader?\",\n",
    "        \"what is moral?\",\n",
    "        \"what is immoral?\",\n",
    "        \"what is altruism?\",\n",
    "        \"ok ... so what is the deﬁnition of morality?\",\n",
    "        \"tell me the deﬁnition of morality , i am quite upset now!\"]\n",
    "\n",
    "encoder_data = tokenize_seq(data)\n",
    "for seq_index in range(len(encoder_data)):\n",
    "    input_seq = encoder_data[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq,e_model,d_model)\n",
    "    print('-')\n",
    "    print('Input sentence:', data[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: i dont know what if it breaks\n",
      "Decoded sentence:  what are you talking about \n",
      "-\n",
      "Input sentence: i wouldnt know anything about that\n",
      "Decoded sentence:  i know that \n",
      "-\n",
      "Input sentence: and lydia telling natalie the truth makes you a victim in what way\n",
      "Decoded sentence:  a lot of <UNK> \n",
      "-\n",
      "Input sentence: no not crazy\n",
      "Decoded sentence:  i dont know \n",
      "-\n",
      "Input sentence: sort of um\n",
      "Decoded sentence:  what do you mean \n",
      "-\n",
      "Input sentence: after i get outta this <UNK> gonna live forever\n",
      "Decoded sentence:  i dont know \n",
      "-\n",
      "Input sentence: i told her she doesnt feel she can do that something about her father not letting her go\n",
      "Decoded sentence:  well i know that you have to go \n",
      "-\n",
      "Input sentence: i didnt mother did\n",
      "Decoded sentence:  yeah \n",
      "-\n",
      "Input sentence: im sorry cole\n",
      "Decoded sentence:  you dont understand \n",
      "-\n",
      "Input sentence: were not dropping it now thats the reason isnt it im a dirty degenerate arent i im not\n",
      "Decoded sentence:  whats the matter of this \n",
      "-\n",
      "Input sentence: when do you deliver your project\n",
      "Decoded sentence:  you know i dont know \n",
      "-\n",
      "Input sentence: no hes more of a dinosaur guys not a dummy though hes juggling alot of balls on this\n",
      "Decoded sentence:  yeah but \n",
      "-\n",
      "Input sentence: a meeting with the prince of <UNK> is impossible\n",
      "Decoded sentence:  the way we have to get knew it \n",
      "-\n",
      "Input sentence: lets get out of here this place makes me sick\n",
      "Decoded sentence:  oh no i think you were just careful \n",
      "-\n",
      "Input sentence: you are going to drive to the frontier\n",
      "Decoded sentence:  what are you talking about \n",
      "-\n",
      "Input sentence: well john adams your cousin has a marvelous gift\n",
      "Decoded sentence:  so what do you mean \n",
      "-\n",
      "Input sentence: do you love me did you love me something i listened to my tape i cant believe ive\n",
      "Decoded sentence:  i think i should go back to the \n",
      "-\n",
      "Input sentence: okay well thats something\n",
      "Decoded sentence:  i dont want to know i was just a little you know that i was thinking i was just for <UNK> of <UNK> to say i was a little all right i done \n",
      "-\n",
      "Input sentence: you been home yet\n",
      "Decoded sentence:  i was supposed to have you to <UNK> \n",
      "-\n",
      "Input sentence: with what\n",
      "Decoded sentence:  i dont know \n",
      "-\n",
      "Input sentence: nothing\n",
      "Decoded sentence:  i dont know \n",
      "-\n",
      "Input sentence: who is he\n",
      "Decoded sentence:  he was just he \n",
      "-\n",
      "Input sentence: sorry this isnt your day\n",
      "Decoded sentence:  oh no \n",
      "-\n",
      "Input sentence: hes awake hes in the airlock hes not wearing a suit\n",
      "Decoded sentence:  well hes just a little one in the world \n",
      "-\n",
      "Input sentence: well id like to travel and maybe go back to school but i really dont <UNK> at a\n",
      "Decoded sentence:  do you know how much he has been on \n",
      "-\n",
      "Input sentence: father\n",
      "Decoded sentence:  well \n",
      "-\n",
      "Input sentence: what\n",
      "Decoded sentence:  i dont know \n",
      "-\n",
      "Input sentence: a lot of what larry says is true they just <UNK> stuff or reverse engineer it and everybody\n",
      "Decoded sentence:  please dont say that to me \n",
      "-\n",
      "Input sentence: cant cant you couldnt you come too i think it would be better for us for us both\n",
      "Decoded sentence:  ill get back to the door \n",
      "-\n",
      "Input sentence: yes general but if you have to shoot somebody you cant\n",
      "Decoded sentence:  oh no \n",
      "-\n",
      "Input sentence: whatever we do we fuck her right\n",
      "Decoded sentence:  good \n",
      "-\n",
      "Input sentence: getchu at that table up yonder\n",
      "Decoded sentence:  who \n",
      "-\n",
      "Input sentence: what is it\n",
      "Decoded sentence:  i dont know \n",
      "-\n",
      "Input sentence: got any kiss\n",
      "Decoded sentence:  yeah i guess \n",
      "-\n",
      "Input sentence: do guys actually believe their lame self serving excuses\n",
      "Decoded sentence:  wheres that \n",
      "-\n",
      "Input sentence: i dont know its not important i mean i dont want pupils they get in the way ive\n",
      "Decoded sentence:  you know i dont know \n",
      "-\n",
      "Input sentence: now phyllis if you dont invite me im coming anyway\n",
      "Decoded sentence:  dont do that \n",
      "-\n",
      "Input sentence: you put the mommy too far away mrs <UNK> has macaroni and glue if you wanna fix it\n",
      "Decoded sentence:  youre right you cant gotta get out of here \n",
      "-\n",
      "Input sentence: jani theres something i want to say before we get there i dont know what the sleeping arrangements\n",
      "Decoded sentence:  thats what you want to know \n",
      "-\n",
      "Input sentence: you give me ulcers\n",
      "Decoded sentence:  you can do that you can keep it out of that ship \n",
      "-\n",
      "Input sentence: hmmm do you miss me\n",
      "Decoded sentence:  no i mean you have to go to work for you \n",
      "-\n",
      "Input sentence: but you took off your captain kidd uniform\n",
      "Decoded sentence:  i thought you were a little girl \n",
      "-\n",
      "Input sentence: i heard ya lady is wild\n",
      "Decoded sentence:  what is it \n",
      "-\n",
      "Input sentence: weve blown the computer elaine set course change\n",
      "Decoded sentence:  hey please \n",
      "-\n",
      "Input sentence: then what did you want to gamble for if youd have beat us out of fifty gs youd\n",
      "Decoded sentence:  im going to be a little little lot of it \n",
      "-\n",
      "Input sentence: hes the reformed dealer though who wanted to turn himself in hes the one that caused frank to\n",
      "Decoded sentence:  look im lot of you know i know \n",
      "-\n",
      "Input sentence: oh no there is nothing i have everything you have given me everything i could possibly want i\n",
      "Decoded sentence:  if you dont mind me any time \n",
      "-\n",
      "Input sentence: its a game son i can explain it pretty easily theres a pitcher\n",
      "Decoded sentence:  the only one in the world \n",
      "-\n",
      "Input sentence: theres been reports of management sexually abusing the artists in this place\n",
      "Decoded sentence:  i dont know how to say that \n",
      "-\n",
      "Input sentence: striker\n",
      "Decoded sentence:  let me see \n",
      "-\n",
      "Input sentence: uh look if youre serious about wanting a job picking apples isnt that boring\n",
      "Decoded sentence:  oh i cant do that \n",
      "-\n",
      "Input sentence: we cant raise the wheel without it\n",
      "Decoded sentence:  how long \n",
      "-\n",
      "Input sentence: what are you doing\n",
      "Decoded sentence:  i got it you got it \n",
      "-\n",
      "Input sentence: oh shes growing up very fast as a matter of fact shes six tomorrow\n",
      "Decoded sentence:  oh yeah \n",
      "-\n",
      "Input sentence: alright go ahead\n",
      "Decoded sentence:  the way to get out of here \n",
      "-\n",
      "Input sentence: what do you think of that guy who works at the theatre you know mark ratner\n",
      "Decoded sentence:  oh he was a little man he was a little guy in the world \n",
      "-\n",
      "Input sentence: you mean id get a chance to talk to her\n",
      "Decoded sentence:  you are a little way you dont know me you dont know me you dont know me you dont know me \n",
      "-\n",
      "Input sentence: interesting\n",
      "Decoded sentence:  what do you mean \n",
      "-\n",
      "Input sentence: what is your first name\n",
      "Decoded sentence:  <UNK> \n",
      "-\n",
      "Input sentence: its a miracle\n",
      "Decoded sentence:  what are you talking about \n",
      "-\n",
      "Input sentence: i dont have time i need to know\n",
      "Decoded sentence:  you know what i mean \n",
      "-\n",
      "Input sentence: so\n",
      "Decoded sentence:  you know what i mean \n",
      "-\n",
      "Input sentence: why dont you call him\n",
      "Decoded sentence:  he was just he \n",
      "-\n",
      "Input sentence: what\n",
      "Decoded sentence:  i dont know \n",
      "-\n",
      "Input sentence: no time for <UNK> kyle we have a level five emergency the commander needs us to get him\n",
      "Decoded sentence:  oh well \n",
      "-\n",
      "Input sentence: mr lovejoy\n",
      "Decoded sentence:  what \n",
      "-\n",
      "Input sentence: the omega 13 why does that sound so familiar\n",
      "Decoded sentence:  he was a guy he was a little man in the world \n",
      "-\n",
      "Input sentence: why not\n",
      "Decoded sentence:  because i dont know \n",
      "-\n",
      "Input sentence: back to jail in my own car ganz got away got all my money it just dont seem\n",
      "Decoded sentence:  i dont know \n",
      "-\n",
      "Input sentence: youre not gonna tell mom are you\n",
      "Decoded sentence:  oh no i dont \n",
      "-\n",
      "Input sentence: on tv ya always get that asshole that jumps behind the couch\n",
      "Decoded sentence:  yes sir \n",
      "-\n",
      "Input sentence: you are a monster zorg\n",
      "Decoded sentence:  you think i should go \n",
      "-\n",
      "Input sentence: you lent it to me in like tenth grade\n",
      "Decoded sentence:  what do you want me to say \n",
      "-\n",
      "Input sentence: ow\n",
      "Decoded sentence:  how long did you get in here \n",
      "-\n",
      "Input sentence: hey boss <UNK> trust is owned by pan illinois which is majority controlled by <UNK> dynamics which is\n",
      "Decoded sentence:  <UNK> of <UNK> to own \n",
      "-\n",
      "Input sentence: daya <UNK> <UNK> <UNK> <UNK>\n",
      "Decoded sentence:  what are you talking about \n",
      "-\n",
      "Input sentence: well i suppose i should just say it its your clothes\n",
      "Decoded sentence:  you know what i mean \n",
      "-\n",
      "Input sentence: thats the guy that got us off the hook with the <UNK> thing\n",
      "Decoded sentence:  he was a little man in the world \n",
      "-\n",
      "Input sentence: motherhood is a very natural instinct for me id like to have a baby myself wouldnt you\n",
      "Decoded sentence:  whats wrong \n",
      "-\n",
      "Input sentence: what is wrong with me\n",
      "Decoded sentence:  i dont know \n",
      "-\n",
      "Input sentence: did you cry when your father died\n",
      "Decoded sentence:  yeah \n",
      "-\n",
      "Input sentence: cool\n",
      "Decoded sentence:  i dont know \n",
      "-\n",
      "Input sentence: is the emperor angry with me\n",
      "Decoded sentence:  the hell are you going to be <UNK> \n",
      "-\n",
      "Input sentence: has it been exposed to any sun before now\n",
      "Decoded sentence:  i dont know \n",
      "-\n",
      "Input sentence: i do not seek forgiveness\n",
      "Decoded sentence:  you were a little than you are not to do you you know how to say \n",
      "-\n",
      "Input sentence: she made it through sere training got a call this morning from\n",
      "Decoded sentence:  you like a little man you know how it is \n",
      "-\n",
      "Input sentence: go to the bathroom\n",
      "Decoded sentence:  right \n",
      "-\n",
      "Input sentence: never heard of it\n",
      "Decoded sentence:  oh no i think you were just a little that you were so sick and i have to get you to be home \n",
      "-\n",
      "Input sentence: two days\n",
      "Decoded sentence:  \n",
      "-\n",
      "Input sentence: mommy where are you\n",
      "Decoded sentence:  oh yes \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: you cant help them right now theyre being cocooned just like the others\n",
      "Decoded sentence:  oh god oh god please you dont know how to get this just go \n",
      "-\n",
      "Input sentence: you dont crap out of specrecon and get another shot without <UNK> from someone up in flag country\n",
      "Decoded sentence:  how long \n",
      "-\n",
      "Input sentence: will scraps be able to sit with us dad\n",
      "Decoded sentence:  well i think he was the only way \n",
      "-\n",
      "Input sentence: we did not leave together\n",
      "Decoded sentence:  well i think he was the only way \n",
      "-\n",
      "Input sentence: the maze you mean the air ducts\n",
      "Decoded sentence:  yeah \n",
      "-\n",
      "Input sentence: yes\n",
      "Decoded sentence:  i dont know \n",
      "-\n",
      "Input sentence: this is maybe worse than you and i are used to what im talking about im talking about\n",
      "Decoded sentence:  <UNK> \n",
      "-\n",
      "Input sentence: you mean to tell me that there is no one who holds a special place in your heart\n",
      "Decoded sentence:  oh yes \n",
      "-\n",
      "Input sentence: good i called you at work today they said you were home sick\n",
      "Decoded sentence:  i dont want to talk about it \n",
      "-\n",
      "Input sentence: laguna jamming custom <UNK>\n",
      "Decoded sentence:  know it wont be \n"
     ]
    }
   ],
   "source": [
    "# Randomly test data in training set\n",
    "for seq_index in np.random.permutation(len(encoder_input_data))[:100]:\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq,e_model,d_model)\n",
    "    print('-')\n",
    "    print('Input sentence:', \" \".join(input_tokens[seq_index][1:-1].tolist()))\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
